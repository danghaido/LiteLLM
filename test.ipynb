{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "350584fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "from LiteLLM.common import CONFIG\n",
    "\n",
    "from phoenix.evals import (\n",
    "    llm_classify,\n",
    "    LiteLLMModel,\n",
    "    llm_generate,\n",
    "    RelevanceEvaluator,\n",
    "    run_evals\n",
    ")\n",
    "\n",
    "import litellm\n",
    "litellm._turn_on_debug()\n",
    "\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = CONFIG.api_key\n",
    "\n",
    "import phoenix as px\n",
    "import pandas as pd\n",
    "\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "from phoenix.trace import SpanEvaluations, using_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dfca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "def normalize_newline(s: str) -> str:\n",
    "    \"\"\"Chuy·ªÉn '\\\\n' literal th√†nh newline th·∫≠t.\"\"\"\n",
    "    return s.replace(\"\\\\n\", \"\\n\") if isinstance(s, str) else s\n",
    "\n",
    "def split_ref_items(s: str):\n",
    "    \"\"\"\n",
    "    T√°ch chu·ªói ref d·∫°ng:\n",
    "    [1] Intel focuses...\n",
    "    [2] AMD Ryzen offers...\n",
    "    [4] Laptop battery life...\n",
    "    Th√†nh list [(id, text), ...]\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = normalize_newline(s)\n",
    "    pattern = re.compile(r\"\\[(\\d+)\\]\\s*(.*?)(?=(?:\\n\\[\\d+\\])|$)\", flags=re.S)\n",
    "    return pattern.findall(s)\n",
    "\n",
    "def explode_refs(df: pd.DataFrame, ref_col: str = \"ref\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Nh·∫≠n DataFrame c√≥ c·ªôt 'ref', tr·∫£ v·ªÅ DataFrame ch·ªâ g·ªìm context.trace_id v√† ref_text.\n",
    "    \"\"\"\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ref_items\"] = tmp[ref_col].apply(split_ref_items)\n",
    "    out = tmp.explode(\"ref_items\", ignore_index=True)\n",
    "    out[[\"ref_id\", \"ref_text\"]] = pd.DataFrame(out[\"ref_items\"].tolist(), index=out.index)\n",
    "    out[\"ref_id\"] = out[\"ref_id\"].astype(int)\n",
    "    return out[[\"ref_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45e8e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Exploded format:\n",
      "                                                                                                                             ref_text\n",
      "0    the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\n",
      "1   its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \n",
      "2   dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\n",
      "3        \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\n",
      "4  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b\n",
      "5  nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "query = SpanQuery().where(\"span_kind == 'CHAIN'\", ).select(\"trace_id\", input=\"input.value\", output=\"output.value\")\n",
    "\n",
    "df = px.Client().query_spans(query, project_name=\"hugging-face\")\n",
    "\n",
    "reference = SpanQuery().where(\"span_kind == 'TOOL'\").select(\"trace_id\", ref=\"prompt.context.preview\")\n",
    "spans_with_docs_df = px.Client().query_spans(reference, project_name=\"hugging-face\")\n",
    "print(len(spans_with_docs_df))\n",
    "\n",
    "document_chunks_df = explode_refs(spans_with_docs_df)\n",
    "print(\"Exploded format:\\n\", document_chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b192a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_questions_template = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{ref_text}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "3 questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided.\"\n",
    "\n",
    "Output the questions in JSON format with the keys question_1, question_2, question_3.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d13026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_newline(s: str) -> str:\n",
    "    \"\"\"Chuy·ªÉn '\\\\n' literal th√†nh newline th·∫≠t.\"\"\"\n",
    "    return s.replace(\"\\\\n\", \"\\n\") if isinstance(s, str) else s\n",
    "\n",
    "def split_ref_items(s: str):\n",
    "    \"\"\"\n",
    "    T√°ch chu·ªói ref d·∫°ng:\n",
    "    [1] Intel focuses...\n",
    "    [2] AMD Ryzen offers...\n",
    "    [4] Laptop battery life...\n",
    "    Th√†nh list [(id, text), ...]\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = normalize_newline(s)\n",
    "    pattern = re.compile(r\"\\[(\\d+)\\]\\s*(.*?)(?=(?:\\n\\[\\d+\\])|$)\", flags=re.S)\n",
    "    return pattern.findall(s)\n",
    "\n",
    "def explode_refs(df: pd.DataFrame, ref_col: str = \"ref\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Nh·∫≠n DataFrame c√≥ c·ªôt 'ref', tr·∫£ v·ªÅ DataFrame ch·ªâ g·ªìm context.trace_id v√† ref_text.\n",
    "    \"\"\"\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ref_items\"] = tmp[ref_col].apply(split_ref_items)\n",
    "    out = tmp.explode(\"ref_items\", ignore_index=True)\n",
    "    out[[\"ref_id\", \"ref_text\"]] = pd.DataFrame(out[\"ref_items\"].tolist(), index=out.index)\n",
    "    out[\"ref_id\"] = out[\"ref_id\"].astype(int)\n",
    "    return out[[\"context.trace_id\", \"ref_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ae3e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_output_parser(response: str, index: int):\n",
    "    s = (response or \"\").strip()\n",
    "    # log v√†i m·∫´u ƒë·∫ßu ƒë·ªÉ b·∫°n xem raw\n",
    "    if index < 3:\n",
    "        print(f\"\\nRAW[{index}]:\\n{repr(s)}\\n\")\n",
    "\n",
    "    if not s:\n",
    "        return {\"__error__\": \"empty\", \"question_1\": None, \"question_2\": None, \"question_3\": None}\n",
    "\n",
    "    # 1) b·ªè code fences ```json ... ```\n",
    "    if s.startswith(\"```\"):\n",
    "        s = re.sub(r\"^```[a-zA-Z]*\\s*\", \"\", s)\n",
    "        s = re.sub(r\"\\s*```$\", \"\", s, flags=re.S)\n",
    "\n",
    "    # 2) l·∫•y block JSON ƒë·∫ßu ti√™n\n",
    "    start, end = s.find(\"{\"), s.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        try:\n",
    "            obj = json.loads(s[start:end+1])\n",
    "            # ƒë·∫£m b·∫£o tr·∫£ v·ªÅ dict c√≥ c√°c key mong mu·ªën\n",
    "            return {\n",
    "                \"question_1\": obj.get(\"question_1\"),\n",
    "                \"question_2\": obj.get(\"question_2\"),\n",
    "                \"question_3\": obj.get(\"question_3\"),\n",
    "            }\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass\n",
    "\n",
    "    # 3) fallback: n·∫øu model tr·∫£ plain text, nh√©t v√†o question_1\n",
    "    return {\"question_1\": s, \"question_2\": None, \"question_3\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81ef4859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n",
      "llm_generate |          | 0/6 (0.0%) | ‚è≥ 00:00<? | ?it/s\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': 'Context information is below.\\n\\n---------------------\\nthe more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:26:59 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\nthe more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\nthe more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRMue-3NKUce-97432f3f884c99fb\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042019,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 6190086200822693000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"```json\\n{\\n  \\\"question_1\\\": \\\"According to the context, which company's flagship products have recently surpassed Intel's in terms of both price and performance?\\\",\\n  \\\"question_2\\\": \\\"What does the phrase 'the more affordable choice' likely refer to in the given context?\\\",\\n  \\\"question_3\\\": \\\"Based on the information provided, what trend can be observed in the competitive landscape between the two companies mentioned?\\\"\\n}\\n```\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 146,\n",
      "    \"completion_tokens\": 90,\n",
      "    \"total_tokens\": 236,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:27:00 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "llm_generate |‚ñà‚ñã        | 1/6 (16.7%) | ‚è≥ 00:01<00:08 |  1.73s/it\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': 'Context information is below.\\n\\n---------------------\\nits efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:27:00 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\nits efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW[0]:\n",
      "'```json\\n{\\n  \"question_1\": \"According to the context, which company\\'s flagship products have recently surpassed Intel\\'s in terms of both price and performance?\",\\n  \"question_2\": \"What does the phrase \\'the more affordable choice\\' likely refer to in the given context?\",\\n  \"question_3\": \"Based on the information provided, what trend can be observed in the competitive landscape between the two companies mentioned?\"\\n}\\n```'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:27:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\nits efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRNSG-3NKUce-97432f4a39f4200c\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042021,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 11450438168943262000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"```json\\n{\\n  \\\"question_1\\\": \\\"What are the two specific processor generations mentioned in the context that feature efficiency cores and a hybrid architecture?\\\",\\n  \\\"question_2\\\": \\\"Identify the term used in the context to describe the combination of different types of processor cores in these generations.\\\",\\n  \\\"question_3\\\": \\\"Based on the context, which event is indicated to have occurred with the mention of 'The launch of'?\\\"\\n}\\n```\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 146,\n",
      "    \"completion_tokens\": 92,\n",
      "    \"total_tokens\": 238,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:27:02 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñé      | 2/6 (33.3%) | ‚è≥ 00:03<00:07 |  1.80s/it\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': 'Context information is below.\\n\\n---------------------\\ndvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:27:02 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\ndvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW[1]:\n",
      "'```json\\n{\\n  \"question_1\": \"What are the two specific processor generations mentioned in the context that feature efficiency cores and a hybrid architecture?\",\\n  \"question_2\": \"Identify the term used in the context to describe the combination of different types of processor cores in these generations.\",\\n  \"question_3\": \"Based on the context, which event is indicated to have occurred with the mention of \\'The launch of\\'?\"\\n}\\n```'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:27:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\ndvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRNth-3NKUce-97432f552ff6200c\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042023,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 901961998825556100,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"```json\\n{\\n  \\\"question_1\\\": \\\"What does the term 'advanced process nodes' refer to in the context of CPU and GPU fabrication?\\\",\\n  \\\"question_2\\\": \\\"Why might a company choose not to own its own fabrication plants (fabs) when producing CPUs and GPUs?\\\",\\n  \\\"question_3\\\": \\\"How does using advanced process nodes contribute to delivering competitive products in the CPU and GPU market?\\\"\\n}\\n```\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 145,\n",
      "    \"completion_tokens\": 87,\n",
      "    \"total_tokens\": 232,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:27:04 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 (50.0%) | ‚è≥ 00:05<00:05 |  1.81s/it\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': 'Context information is below.\\n\\n---------------------\\n\"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:27:04 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\n\"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW[2]:\n",
      "'```json\\n{\\n  \"question_1\": \"What does the term \\'advanced process nodes\\' refer to in the context of CPU and GPU fabrication?\",\\n  \"question_2\": \"Why might a company choose not to own its own fabrication plants (fabs) when producing CPUs and GPUs?\",\\n  \"question_3\": \"How does using advanced process nodes contribute to delivering competitive products in the CPU and GPU market?\"\\n}\\n```'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\n\"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRPVQ-3NKUce-97432f6079e20a89\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042025,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 2426906414981683000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"```json\\n{\\n  \\\"question_1\\\": \\\"What feature of AMD's Ryzen processors has contributed to their popularity among content creators?\\\",\\n  \\\"question_2\\\": \\\"How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?\\\",\\n  \\\"question_3\\\": \\\"Based on the context, which type of users are likely to find AMD's Ryzen processors particularly appealing?\\\"\\n}\\n```\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 142,\n",
      "    \"completion_tokens\": 80,\n",
      "    \"total_tokens\": 222,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:27:05 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:05 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 (50.0%) | ‚è≥ 00:07<00:05 |  1.81s/it \u001b[92m20:27:06 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:27:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 (66.7%) | ‚è≥ 00:07<00:03 |  1.80s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process was interrupted. The return value will be incomplete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LiteLLMModel(\n",
    "    model=\"huggingface/together/Qwen/Qwen2.5-7B-Instruct\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "questions_df = llm_generate(\n",
    "    dataframe=document_chunks_df,\n",
    "    template=generate_questions_template,\n",
    "    model=model,\n",
    "    output_parser=robust_output_parser,\n",
    "    concurrency=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45d86f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>question_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What feature of AMD's Ryzen processors has contributed to their popularity among content creators?</td>\n",
       "      <td>How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?</td>\n",
       "      <td>Based on the context, which type of users are likely to find AMD's Ryzen processors particularly appealing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was the impact of AMD's EPYC server processors on the data center market according to the given context?</td>\n",
       "      <td>Based on the information provided, what specific feature of AMD's EPYC processors is mentioned as a key factor in disrupting the market?</td>\n",
       "      <td>Summarize the main point of the given context in one sentence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two companies mentioned in the context that are expanding into GPU technologies?</td>\n",
       "      <td>Which company is focusing on Radeon graphics according to the context?</td>\n",
       "      <td>What is the focus of Intel's expansion into GPU technologies based on the given information?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the context, what are both AMD and Intel focusing on in the future?</td>\n",
       "      <td>How does the context describe the current state of price and raw performance between AMD and Intel?</td>\n",
       "      <td>Summarize the main points mentioned about AMD and Intel's future strategies in the given context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the term 'fabs' refer to in the context of semiconductor manufacturing, and why might a company like AMD not own its own fabs?</td>\n",
       "      <td>How has AMD's performance in the desktop market changed in recent years, and what does the context suggest about Intel's position in this market?</td>\n",
       "      <td>Based on the given context, what can be inferred about the competitive landscape between AMD and Intel in the desktop market?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 question_1  \\\n",
       "0                                        What feature of AMD's Ryzen processors has contributed to their popularity among content creators?   \n",
       "1                             What was the impact of AMD's EPYC server processors on the data center market according to the given context?   \n",
       "2                                             What are the two companies mentioned in the context that are expanding into GPU technologies?   \n",
       "3                                                          According to the context, what are both AMD and Intel focusing on in the future?   \n",
       "4  What does the term 'fabs' refer to in the context of semiconductor manufacturing, and why might a company like AMD not own its own fabs?   \n",
       "\n",
       "                                                                                                                                          question_2  \\\n",
       "0                                                      How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?   \n",
       "1           Based on the information provided, what specific feature of AMD's EPYC processors is mentioned as a key factor in disrupting the market?   \n",
       "2                                                                             Which company is focusing on Radeon graphics according to the context?   \n",
       "3                                                How does the context describe the current state of price and raw performance between AMD and Intel?   \n",
       "4  How has AMD's performance in the desktop market changed in recent years, and what does the context suggest about Intel's position in this market?   \n",
       "\n",
       "                                                                                                                      question_3  \n",
       "0                    Based on the context, which type of users are likely to find AMD's Ryzen processors particularly appealing?  \n",
       "1                                                                 Summarize the main point of the given context in one sentence.  \n",
       "2                                   What is the focus of Intel's expansion into GPU technologies based on the given information?  \n",
       "3                              Summarize the main points mentioned about AMD and Intel's future strategies in the given context.  \n",
       "4  Based on the given context, what can be inferred about the competitive landscape between AMD and Intel in the desktop market?  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b730d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataframe of the questions and the document chunks\n",
    "questions_with_document_chunk_df = pd.concat([questions_df, document_chunks_df], axis=1)\n",
    "questions_with_document_chunk_df = questions_with_document_chunk_df.melt(\n",
    "    id_vars=[\"ref_text\"], value_name=\"question\"\n",
    ").drop(\"variable\", axis=1)\n",
    "# If the above step was interrupted, there might be questions missing. Let's run this to clean up the dataframe.\n",
    "questions_with_document_chunk_df = questions_with_document_chunk_df[\n",
    "    questions_with_document_chunk_df[\"question\"].notnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a958888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_text</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>What feature of AMD's Ryzen processors has contributed to their popularity among content creators?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>What was the impact of AMD's EPYC server processors on the data center market according to the given context?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>What are the two companies mentioned in the context that are expanding into GPU technologies?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi</td>\n",
       "      <td>According to the context, what are both AMD and Intel focusing on in the future?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong</td>\n",
       "      <td>What does the term 'fabs' refer to in the context of semiconductor manufacturing, and why might a company like AMD not own its own fabs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>What are the two companies mentioned in the context that are expanding into GPU technologies?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>What feature of AMD's Ryzen processors has contributed to their popularity among content creators?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>What was the impact of AMD's EPYC server processors on the data center market according to the given context?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t</td>\n",
       "      <td>What challenge has el faced in adopting newer nanometer processes?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                            ref_text  \\\n",
       "0        \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "1  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "2  nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "3  ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi   \n",
       "4  s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong    \n",
       "5  nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "6        \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "7  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "8   el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t   \n",
       "9        \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "\n",
       "                                                                                                                                   question  \n",
       "0                                        What feature of AMD's Ryzen processors has contributed to their popularity among content creators?  \n",
       "1                             What was the impact of AMD's EPYC server processors on the data center market according to the given context?  \n",
       "2                                             What are the two companies mentioned in the context that are expanding into GPU technologies?  \n",
       "3                                                          According to the context, what are both AMD and Intel focusing on in the future?  \n",
       "4  What does the term 'fabs' refer to in the context of semiconductor manufacturing, and why might a company like AMD not own its own fabs?  \n",
       "5                                             What are the two companies mentioned in the context that are expanding into GPU technologies?  \n",
       "6                                        What feature of AMD's Ryzen processors has contributed to their popularity among content creators?  \n",
       "7                             What was the impact of AMD's EPYC server processors on the data center market according to the given context?  \n",
       "8                                                                        What challenge has el faced in adopting newer nanometer processes?  \n",
       "9                                             How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_with_document_chunk_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenTelemetry Tracing Details\n",
      "|  Phoenix Project: hugging-face\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n",
      "Running 27 questions‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG batch:   0%|          | 0/27 [00:00<?, ?q/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "RAG batch:   0%|          | 0/27 [00:12<?, ?q/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     _ = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     43\u001b[39m span.set_attribute(\u001b[33m\"\u001b[39m\u001b[33minput.value\u001b[39m\u001b[33m\"\u001b[39m, query)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# t·∫°o prompt t·ª´ RAG (l·∫•y context top-k cho c√¢u h·ªèi)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     prompt = \u001b[43mbuild_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# g·ªçi model\u001b[39;00m\n\u001b[32m     50\u001b[39m     msg = ResponseInput(prompt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\tools\\rag.py:26\u001b[39m, in \u001b[36mbuild_prompt\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m     24\u001b[39m span.set_attribute(\u001b[33m\"\u001b[39m\u001b[33minput.value\u001b[39m\u001b[33m\"\u001b[39m, query)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     chunks = \u001b[43mretrieve_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     context = (\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks, start=\u001b[32m1\u001b[39m)])\n\u001b[32m     29\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m[1] (no context)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m     )\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# log ids + preview context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\tools\\retriever.py:61\u001b[39m, in \u001b[36mretrieve_chunks\u001b[39m\u001b[34m(query, top_k, fetch_k)\u001b[39m\n\u001b[32m     58\u001b[39m     span.add_event(notification)\n\u001b[32m     60\u001b[39m pairs = [[query, t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m scores = \u001b[43mreranker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m ranked = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(ids, docs, metas, scores), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m3\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     64\u001b[39m top = ranked[:top_k]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\FlagEmbedding\\abc\\inference\\AbsReranker.py:218\u001b[39m, in \u001b[36mAbsReranker.compute_score\u001b[39m\u001b[34m(self, sentence_pairs, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m sentence_pairs = \u001b[38;5;28mself\u001b[39m.get_detailed_inputs(sentence_pairs)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentence_pairs, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.target_devices) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_score_single_gpu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43msentence_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_devices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28mself\u001b[39m.pool = \u001b[38;5;28mself\u001b[39m.start_multi_process_pool()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\FlagEmbedding\\inference\\reranker\\encoder_only\\base.py:187\u001b[39m, in \u001b[36mBaseReranker.compute_score_single_gpu\u001b[39m\u001b[34m(self, sentence_pairs, batch_size, query_max_length, max_length, normalize, device, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     sentences_batch = all_inputs_sorted[start_index:start_index + batch_size]\n\u001b[32m    180\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer.pad(\n\u001b[32m    181\u001b[39m         sentences_batch,\n\u001b[32m    182\u001b[39m         padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    183\u001b[39m         return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    184\u001b[39m         **kwargs\n\u001b[32m    185\u001b[39m     ).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.logits.view(-\u001b[32m1\u001b[39m, ).float()\n\u001b[32m    188\u001b[39m     all_scores.extend(scores.cpu().numpy().tolist())\n\u001b[32m    190\u001b[39m all_scores = [all_scores[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np.argsort(length_sorted_idx)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:1189\u001b[39m, in \u001b[36mXLMRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1172\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1174\u001b[39m \u001b[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1185\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1186\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1187\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1201\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:859\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    853\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    857\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    872\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    873\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:610\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    606\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    608\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:548\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[39m\n\u001b[32m    545\u001b[39m     attention_output = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    546\u001b[39m     outputs = outputs + cross_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:251\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:556\u001b[39m, in \u001b[36mXLMRobertaLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:471\u001b[39m, in \u001b[36mXLMRobertaIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from opentelemetry.trace import Status, StatusCode\n",
    "# from Phoenix.trace.tracing import tracer\n",
    "\n",
    "# from LiteLLM.lite import LiteLLMClient\n",
    "# from LiteLLM.Response import ResponseInput\n",
    "# from tools.rag import build_prompt\n",
    "\n",
    "# # --------- CONFIG ---------\n",
    "# # df ngu·ªìn: l·∫•y c·ªôt \"question\"\n",
    "# SOURCE_DF = questions_with_document_chunk_df   # <- ƒë√£ c√≥ s·∫µn ·ªü ph√≠a b·∫°n\n",
    "# QUESTION_COL = \"question\"\n",
    "# TOP_K = 3          # build_prompt(query, top_k=TOP_K)\n",
    "# MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")  # ƒë·ªïi n·∫øu c·∫ßn\n",
    "# TEMP = float(os.getenv(\"LLM_TEMP\", \"0.2\"))\n",
    "# MAX_NUM = None     # gi·ªõi h·∫°n s·ªë c√¢u ch·∫°y (None = ch·∫°y h·∫øt)\n",
    "# # --------------------------\n",
    "\n",
    "# def main():\n",
    "#     # Chu·∫©n b·ªã danh s√°ch c√¢u h·ªèi\n",
    "#     queries = (\n",
    "#         SOURCE_DF[QUESTION_COL]\n",
    "#         .dropna()\n",
    "#         .map(lambda s: str(s).strip())\n",
    "#         .loc[lambda s: s.ne(\"\")]\n",
    "#         .tolist()\n",
    "#     )\n",
    "#     if MAX_NUM is not None:\n",
    "#         queries = queries[:MAX_NUM]\n",
    "\n",
    "#     print(f\"Running {len(queries)} questions‚Ä¶\")\n",
    "\n",
    "#     # Client LLM (tu·ª≥ m√¥i tr∆∞·ªùng b·∫°n c·∫•u h√¨nh API key/endpoint)\n",
    "#     client = LiteLLMClient()\n",
    "\n",
    "#     rows = []  # l∆∞u k·∫øt qu·∫£\n",
    "\n",
    "#     for i, query in enumerate(tqdm(queries, desc=\"RAG batch\", unit=\"q\")):\n",
    "#         with tracer.start_as_current_span(\"Thought\") as span:\n",
    "#             span.set_attribute(\"openinference.span.kind\", \"CHAIN\")\n",
    "#             span.set_attribute(\"input.value\", query)\n",
    "\n",
    "#             try:\n",
    "#                 # t·∫°o prompt t·ª´ RAG (l·∫•y context top-k cho c√¢u h·ªèi)\n",
    "#                 prompt = build_prompt(query, top_k=TOP_K)\n",
    "\n",
    "#                 # g·ªçi model\n",
    "#                 msg = ResponseInput(prompt)\n",
    "#                 resp = client.complete([msg])           # one-by-one (·ªïn ƒë·ªãnh)\n",
    "#                 answer = resp.transform()\n",
    "#                 usage = None\n",
    "#                 try:\n",
    "#                     usage = resp.usage()\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "\n",
    "#                 # log l√™n trace\n",
    "#                 span.set_attribute(\"output.value\", (answer or \"\")[:400])\n",
    "#                 span.set_status(Status(StatusCode.OK))\n",
    "\n",
    "#                 rows.append({\n",
    "#                     \"index\": i,\n",
    "#                     \"question\": query,\n",
    "#                     \"prompt\": prompt,\n",
    "#                     \"answer\": answer,\n",
    "#                     \"usage\": usage,\n",
    "#                     \"error\": None,\n",
    "#                 })\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 # ghi l·ªói nh∆∞ng kh√¥ng d·ª´ng batch\n",
    "#                 span.record_exception(e)\n",
    "#                 span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "#                 rows.append({\n",
    "#                     \"index\": i,\n",
    "#                     \"question\": query,\n",
    "#                     \"prompt\": None,\n",
    "#                     \"answer\": None,\n",
    "#                     \"usage\": None,\n",
    "#                     \"error\": str(e),\n",
    "#                 })\n",
    "\n",
    "#     # Th√†nh DataFrame k·∫øt qu·∫£\n",
    "#     results_df = pd.DataFrame(rows)\n",
    "#     pd.set_option(\"display.max_colwidth\", None)\n",
    "#     print(\"\\n=== SAMPLE RESULTS ===\")\n",
    "#     print(results_df.head(10)[[\"question\", \"answer\", \"error\"]])\n",
    "\n",
    "#     # (tu·ª≥ ch·ªçn) l∆∞u file\n",
    "#     # results_df.to_csv(\"rag_batch_results.csv\", index=False)\n",
    "#     return results_df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     _ = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9062b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['context.span_id', 'context.trace_id', 'input.value',\n",
      "       'retrieval.documents'],\n",
      "      dtype='object')\n",
      "    context.span_id                  context.trace_id  \\\n",
      "0  99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
      "1  0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
      "2  c149e18e115a10db  012ba572ab94742d40164868224f2599   \n",
      "3  18aee44228d90a80  6027ca1f7c45cb40e05605d3b6bb73b5   \n",
      "\n",
      "                                 input.value  \\\n",
      "0  Can you give me any details about Samsung   \n",
      "1       I want to know information about AMD   \n",
      "2            I want to compare AMD and Intel   \n",
      "3                      Is AMD has multi core   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           retrieval.documents  \n",
      "0                  [{'document': {'id': '09b3a531-26e4-4023-8b0a-ae6af3affc40', 'score': 0.0019389385, 'content': ' the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per'}}, {'document': {'id': 'a0149ea1-fb03-4f53-be3a-891702d05314', 'score': 0.000306159, 'content': ' its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\n",
      "    \"The launch of '}}, {'document': {'id': 'bb1046e5-feb9-4bb8-8546-95829d996604', 'score': 0.0001892578, 'content': 'dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"'}}]  \n",
      "1         [{'document': {'id': '6e13a8d5-0deb-4982-8f6a-601164f66a73', 'score': 0.0256577818, 'content': '\n",
      "    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat'}}, {'document': {'id': '825177e1-6c77-4de7-bb02-c332e8dbaf54', 'score': 0.0070900542, 'content': 'nerations. \"\n",
      "    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b'}}, {'document': {'id': 'a23b9fae-a936-451e-b8d5-573ab30f08dd', 'score': 0.004895068000000001, 'content': 'nd pre-built PCs. \"\n",
      "    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun'}}]  \n",
      "2                 [{'document': {'id': 'def89f0c-d7d3-4c57-964a-4689cfda6ad3', 'score': 0.8576124011, 'content': 'ntel‚Äôs in both price and raw performance. \"\n",
      "    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi'}}, {'document': {'id': '207f6d67-3dbf-4aea-88dc-c9df94f43b43', 'score': 0.7549440774, 'content': 's without owning its own fabs. \"\n",
      "    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong '}}, {'document': {'id': 'a23b9fae-a936-451e-b8d5-573ab30f08dd', 'score': 0.7357937173, 'content': 'nd pre-built PCs. \"\n",
      "    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun'}}]  \n",
      "3  [{'document': {'id': '6e13a8d5-0deb-4982-8f6a-601164f66a73', 'score': 0.9939568673, 'content': '\n",
      "    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat'}}, {'document': {'id': '825177e1-6c77-4de7-bb02-c332e8dbaf54', 'score': 0.38046522790000004, 'content': 'nerations. \"\n",
      "    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b'}}, {'document': {'id': 'db538914-83d3-45b7-81e9-6cdd203b2c7c', 'score': 0.027637798800000002, 'content': 'el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t'}}]  \n"
     ]
    }
   ],
   "source": [
    "client = px.Client()\n",
    "\n",
    "query = (\n",
    "    SpanQuery()\n",
    "    .where(\"span_kind == 'RETRIEVER' and evals['relevance'].label is None   \")\n",
    "    .select(\"context.span_id\", \"context.trace_id\", \"input.value\", \"retrieval.documents\")\n",
    ")\n",
    "\n",
    "df = client.query_spans(query, project_name=\"hugging-face\")\n",
    "df = df.reset_index().rename(columns={\"index\": \"context.span_id\"})\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a395158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['context.span_id', 'context.trace_id', 'input.value',\n",
      "       'retrieval.documents'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_exploded = df.explode(\"retrieval.documents\", ignore_index=True)\n",
    "\n",
    "print(df_exploded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ee6ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['context.span_id', 'context.trace_id', 'input.value', 'document.id',\n",
      "       'document.score', 'document.content'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# normalize nested dict\n",
    "docs = pd.json_normalize(df_exploded[\"retrieval.documents\"])\n",
    "\n",
    "# g·ªôp l·∫°i v·ªõi trace_id\n",
    "retrieved_documents_df = pd.concat([df_exploded.drop(columns=[\"retrieval.documents\"]), docs], axis=1)\n",
    "\n",
    "print(retrieved_documents_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9a950b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents_df = retrieved_documents_df.rename(\n",
    "    columns={\n",
    "        \"document.content\": \"reference\",\n",
    "        \"input.value\": \"input\"   # n·∫øu b·∫°n c√≥ c·ªôt input.value th√¨ rename\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f23c3bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document.id</th>\n",
       "      <th>document.score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6779f6adbac89657</td>\n",
       "      <td>988c1a51b760595beade111fa37c03bb</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.025658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6779f6adbac89657</td>\n",
       "      <td>988c1a51b760595beade111fa37c03bb</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.007090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6779f6adbac89657</td>\n",
       "      <td>988c1a51b760595beade111fa37c03bb</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.004895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a0ec6243d4457141</td>\n",
       "      <td>a2e42b61b13ad2a0b66de97ce42993e5</td>\n",
       "      <td>I want some compare between AMD and Intel</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.830985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a0ec6243d4457141</td>\n",
       "      <td>a2e42b61b13ad2a0b66de97ce42993e5</td>\n",
       "      <td>I want some compare between AMD and Intel</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.743740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a0ec6243d4457141</td>\n",
       "      <td>a2e42b61b13ad2a0b66de97ce42993e5</td>\n",
       "      <td>I want some compare between AMD and Intel</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.736834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0e559a01e10f0f40</td>\n",
       "      <td>4c8aaf9e347a8e0c08b9a196e00b4347</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.993957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0e559a01e10f0f40</td>\n",
       "      <td>4c8aaf9e347a8e0c08b9a196e00b4347</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.380465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0e559a01e10f0f40</td>\n",
       "      <td>4c8aaf9e347a8e0c08b9a196e00b4347</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t</td>\n",
       "      <td>db538914-83d3-45b7-81e9-6cdd203b2c7c</td>\n",
       "      <td>0.027638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0c8c078f8afc59e3</td>\n",
       "      <td>572ce6d156e7bec8e0bf40c24b06954b</td>\n",
       "      <td>What feature of AMD's Ryzen processors has contributed to their popularity among content creators?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    context.span_id                  context.trace_id  \\\n",
       "0  6779f6adbac89657  988c1a51b760595beade111fa37c03bb   \n",
       "1  6779f6adbac89657  988c1a51b760595beade111fa37c03bb   \n",
       "2  6779f6adbac89657  988c1a51b760595beade111fa37c03bb   \n",
       "3  a0ec6243d4457141  a2e42b61b13ad2a0b66de97ce42993e5   \n",
       "4  a0ec6243d4457141  a2e42b61b13ad2a0b66de97ce42993e5   \n",
       "5  a0ec6243d4457141  a2e42b61b13ad2a0b66de97ce42993e5   \n",
       "6  0e559a01e10f0f40  4c8aaf9e347a8e0c08b9a196e00b4347   \n",
       "7  0e559a01e10f0f40  4c8aaf9e347a8e0c08b9a196e00b4347   \n",
       "8  0e559a01e10f0f40  4c8aaf9e347a8e0c08b9a196e00b4347   \n",
       "9  0c8c078f8afc59e3  572ce6d156e7bec8e0bf40c24b06954b   \n",
       "\n",
       "                                                                                                input  \\\n",
       "0                                                                I want to know information about AMD   \n",
       "1                                                                I want to know information about AMD   \n",
       "2                                                                I want to know information about AMD   \n",
       "3                                                           I want some compare between AMD and Intel   \n",
       "4                                                           I want some compare between AMD and Intel   \n",
       "5                                                           I want some compare between AMD and Intel   \n",
       "6                                                                               Is AMD has multi core   \n",
       "7                                                                               Is AMD has multi core   \n",
       "8                                                                               Is AMD has multi core   \n",
       "9  What feature of AMD's Ryzen processors has contributed to their popularity among content creators?   \n",
       "\n",
       "                                                                                                                           reference  \\\n",
       "0  \\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "1  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "2  nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "3  ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi   \n",
       "4  s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong    \n",
       "5  nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "6  \\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "7  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "8   el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t   \n",
       "9                                                                                                                                NaN   \n",
       "\n",
       "                            document.id  document.score  \n",
       "0  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.025658  \n",
       "1  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.007090  \n",
       "2  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.004895  \n",
       "3  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.830985  \n",
       "4  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.743740  \n",
       "5  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.736834  \n",
       "6  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.993957  \n",
       "7  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.380465  \n",
       "8  db538914-83d3-45b7-81e9-6cdd203b2c7c        0.027638  \n",
       "9                                   NaN             NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc4e111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n",
      "run_evals |          | 0/12 (0.0%) | ‚è≥ 00:00<? | ?it/s\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]:  the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:03 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]:  the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]:  the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRgyb-3NKUce-974330cfd9f55813\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042083,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 9746349738715994000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for details about Samsung. The reference text mentions \\\"flagship products\\\" related to Samsung, even though it does not provide specific details. However, it does indicate that Samsung's products have competed with Intel's in terms of price and performance. This information, while not detailed, does provide some context about Samsung's product range and market position.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 292,\n",
      "    \"completion_tokens\": 79,\n",
      "    \"total_tokens\": 371,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:04 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñä         | 1/12 (8.3%) | ‚è≥ 00:01<00:19 |  1.74s/it\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:04 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:05 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRhQu-62bZhn-974330d9aa0b07d3\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042085,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 10198809585572870000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for details about Samsung. The reference text does not mention Samsung at all, instead it discusses efficiency cores and hybrid architecture in the context of processor generations like Alder Lake and Raptor Lake. This information is not relevant to Samsung as a company or brand.\\nLABEL: unrelated\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 292,\n",
      "    \"completion_tokens\": 62,\n",
      "    \"total_tokens\": 354,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:06 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñã        | 2/12 (16.7%) | ‚è≥ 00:03<00:15 |  1.54s/it\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]: dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:06 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]: dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Can you give me any details about Samsung\\n    ************\\n    [Reference text]: dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRhpm-62bZhn-974330e24a7f394a\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042086,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 10569097484198935000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for details about Samsung. The reference text mentions Samsung's process of fabricating CPUs and GPUs using advanced process nodes and not owning its own fabs. This information provides insight into Samsung's manufacturing strategy, which can be considered relevant details about the company. However, the text does not provide a comprehensive overview of Samsung, so it is limited in scope.\\n\\nLABEL: \\\"relevant\\\"\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 290,\n",
      "    \"completion_tokens\": 82,\n",
      "    \"total_tokens\": 372,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:07 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà‚ñå       | 3/12 (25.0%) | ‚è≥ 00:04<00:13 |  1.54s/it\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:07 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:07 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:08 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRiSS-62bZhn-974330ef2ffe0a89\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042088,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 16804796285401272000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and provides specific information about their performance. This information can help answer the question by providing details about AMD's products and their capabilities.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 288,\n",
      "    \"completion_tokens\": 50,\n",
      "    \"total_tokens\": 338,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:09 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñé      | 4/12 (33.3%) | ‚è≥ 00:06<00:13 |  1.67s/it\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:09 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:09 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:10 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:10 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRirj-3NKUce-974330f78f2ee601\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042090,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 14232984930832134000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 292,\n",
      "    \"completion_tokens\": 57,\n",
      "    \"total_tokens\": 349,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:10 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:10 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:10 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 (41.7%) | ‚è≥ 00:08<00:11 |  1.60s/it\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:11 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:11 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRjHk-3NKUce-97433100cf290837\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042091,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 11563410183023241000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about AMD. The reference text mentions AMD specifically, noting its focus on Radeon graphics. This information is directly relevant to answering the question about AMD.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 291,\n",
      "    \"completion_tokens\": 42,\n",
      "    \"total_tokens\": 333,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:12 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà     | 6/12 (50.0%) | ‚è≥ 00:09<00:09 |  1.51s/it\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:12 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:12 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:13 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRjfz-62bZhn-974331092ce45878\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042093,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 10220665516651667000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both AMD and Intel, discussing their performance and pricing. It also hints at their future focus areas. While the text does not provide a detailed comparison, it does mention both companies, which is a starting point for a comparison. Therefore, the reference text contains information relevant to the question.\\n\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 294,\n",
      "    \"completion_tokens\": 78,\n",
      "    \"total_tokens\": 372,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:13 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:13 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:13 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7/12 (58.3%) | ‚è≥ 00:10<00:07 |  1.53s/it\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:14 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRk8u-62bZhn-97433112eb825878\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042094,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 16043007731865688000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The reference text mentions both AMD and Intel, which are the companies the question is comparing. Although the text does not provide specific details for comparison, it does mention both companies, indicating that the reference text is related to the question.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 289,\n",
      "    \"completion_tokens\": 54,\n",
      "    \"total_tokens\": 343,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:15 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 (66.7%) | ‚è≥ 00:12<00:05 |  1.48s/it\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:15 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:15 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to compare AMD and Intel\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:16 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRkZo-3NKUce-9743311ba9585878\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042096,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 11346625284309721000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both companies and their expansion into GPU technologies, which is directly related to comparing their product lines and strategic directions. This information can help in understanding the competitive landscape between AMD and Intel.\\n\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 291,\n",
      "    \"completion_tokens\": 57,\n",
      "    \"total_tokens\": 348,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:16 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:16 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 9/12 (75.0%) | ‚è≥ 00:13<00:04 |  1.54s/it\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:17 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:17 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRm49-3NKUce-97433125e8d00a89\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042097,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 13785515889140349000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks if AMD has multi-core capabilities. The reference text mentions that AMD's Ryzen processors have strong multi-core performance. This directly addresses whether AMD supports multi-core, providing a clear affirmative answer to the question.\\nLABEL: \\\"relevant\\\"\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 286,\n",
      "    \"completion_tokens\": 53,\n",
      "    \"total_tokens\": 339,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:28:18 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:15<00:03 |  1.51s/it\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:18 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:18 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:19 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:16<00:03 |  1.51s/it \u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 1: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:19 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:19 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:20 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:17<00:03 |  1.51s/it \u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:20 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 2: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:20 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:21 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:17<00:03 |  1.51s/it \u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:21 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 3: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:21 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:18<00:03 |  1.51s/it \u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:21 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 4: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:21 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:22 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:19<00:03 |  1.51s/it \u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:22 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 5: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:22 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:19<00:03 |  1.51s/it \u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:22 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:22 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 6: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:23 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:20<00:03 |  1.51s/it \u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:23 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 7: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:23 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:24 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:20<00:03 |  1.51s/it \u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:24 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 8: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:24 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:21<00:03 |  1.51s/it \u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:24 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:24 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 9: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:25 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:22<00:03 |  1.51s/it \u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:28:25 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:28:25 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 10: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:28:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: Is AMD has multi core\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:28:26 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m20:28:26 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m20:28:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m20:28:26 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 (83.3%) | ‚è≥ 00:23<00:03 |  1.51s/it "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Retries exhausted after 11 attempts: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals import (\n",
    "    RelevanceEvaluator,\n",
    "    run_evals,\n",
    ")\n",
    "\n",
    "relevance_evaluator = RelevanceEvaluator(model)\n",
    "\n",
    "retrieved_documents_relevance_df = run_evals(\n",
    "    evaluators=[relevance_evaluator],\n",
    "    dataframe=retrieved_documents_df,\n",
    "    provide_explanation=True,\n",
    "    concurrency=20,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7dedb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and provides specific information about their performance. This information can help answer the question by providing details about AMD's products and their capabilities.\\n\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD specifically, noting its focus on Radeon graphics. This information is directly relevant to answering the question about AMD.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>EXPLANATION: The question asks for a comparison between AMD and Intel. The reference text mentions both companies and provides a brief comparison in terms of price and raw performance. It also mentions their future focus areas. While the text does not provide a detailed comparison, it does contain relevant information that can help answer the question.\\n\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>EXPLANATION: The question asks for a comparison between AMD and Intel. The reference text mentions both AMD and Intel, discussing AMD's momentum in the desktop market and Intel's strong position. This information can be used to compare the two companies, even though the text does not explicitly state a comparison. Therefore, the reference text contains relevant information to answer the question.\\n\\nLABEL: \"relevant\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  score  \\\n",
       "0  relevant      1   \n",
       "1  relevant      1   \n",
       "2  relevant      1   \n",
       "3  relevant      1   \n",
       "4  relevant      1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                            explanation  \n",
       "0                                                                                                                            EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and provides specific information about their performance. This information can help answer the question by providing details about AMD's products and their capabilities.\\n\\nLABEL: relevant  \n",
       "1                                                                                                                             EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant  \n",
       "2                                                                                                                                                                                              EXPLANATION: The question asks for information about AMD. The reference text mentions AMD specifically, noting its focus on Radeon graphics. This information is directly relevant to answering the question about AMD.\\nLABEL: relevant  \n",
       "3                                                 EXPLANATION: The question asks for a comparison between AMD and Intel. The reference text mentions both companies and provides a brief comparison in terms of price and raw performance. It also mentions their future focus areas. While the text does not provide a detailed comparison, it does contain relevant information that can help answer the question.\\n\\nLABEL: relevant  \n",
       "4  EXPLANATION: The question asks for a comparison between AMD and Intel. The reference text mentions both AMD and Intel, discussing AMD's momentum in the desktop market and Intel's strong position. This information can be used to compare the two companies, even though the text does not explicitly state a comparison. Therefore, the reference text contains relevant information to answer the question.\\n\\nLABEL: \"relevant\"  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_relevance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "980043dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context.span_id', 'context.trace_id', 'input', 'document.id',\n",
       "       'document.score', 'reference'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e31bb370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>document.id</th>\n",
       "      <th>document.score</th>\n",
       "      <th>reference</th>\n",
       "      <th>eval_label</th>\n",
       "      <th>eval_score</th>\n",
       "      <th>eval_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>79a556783429c63d0be2f78cc668e5b2</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>09b3a531-26e4-4023-8b0a-ae6af3affc40</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for details about Samsung. The reference text mentions \"flagship products\" related to Samsung, even though it does not provide specific details. However, it does indicate that Samsung's products have competed with Intel's in terms of price and performance. This information, while not detailed, does provide some context about Samsung's product range and market position.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>79a556783429c63d0be2f78cc668e5b2</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EXPLANATION: The question asks for details about Samsung. The reference text does not mention Samsung at all, instead it discusses efficiency cores and hybrid architecture in the context of processor generations like Alder Lake and Raptor Lake. This information is not relevant to Samsung as a company or brand.\\nLABEL: unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>79a556783429c63d0be2f78cc668e5b2</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>bb1046e5-feb9-4bb8-8546-95829d996604</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for details about Samsung. The reference text mentions Samsung's process of fabricating CPUs and GPUs using advanced process nodes and not owning its own fabs. This information provides insight into Samsung's manufacturing strategy, which can be considered relevant details about the company. However, the text does not provide a comprehensive overview of Samsung, so it is limited in scope.\\n\\nLABEL: \"relevant\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>8c37e9fe85f8bf62b84186e36a95eece</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.025658</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and provides specific information about their performance. This information can help answer the question by providing details about AMD's products and their capabilities.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>8c37e9fe85f8bf62b84186e36a95eece</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>8c37e9fe85f8bf62b84186e36a95eece</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD specifically, noting its focus on Radeon graphics. This information is directly relevant to answering the question about AMD.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c149e18e115a10db</td>\n",
       "      <td>012ba572ab94742d40164868224f2599</td>\n",
       "      <td>I want to compare AMD and Intel</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.857612</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both AMD and Intel, discussing their performance and pricing. It also hints at their future focus areas. While the text does not provide a detailed comparison, it does mention both companies, which is a starting point for a comparison. Therefore, the reference text contains information relevant to the question.\\n\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c149e18e115a10db</td>\n",
       "      <td>012ba572ab94742d40164868224f2599</td>\n",
       "      <td>I want to compare AMD and Intel</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.754944</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The reference text mentions both AMD and Intel, which are the companies the question is comparing. Although the text does not provide specific details for comparison, it does mention both companies, indicating that the reference text is related to the question.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c149e18e115a10db</td>\n",
       "      <td>012ba572ab94742d40164868224f2599</td>\n",
       "      <td>I want to compare AMD and Intel</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.735794</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both companies and their expansion into GPU technologies, which is directly related to comparing their product lines and strategic directions. This information can help in understanding the competitive landscape between AMD and Intel.\\n\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18aee44228d90a80</td>\n",
       "      <td>6027ca1f7c45cb40e05605d3b6bb73b5</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.993957</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks if AMD has multi-core capabilities. The reference text mentions that AMD's Ryzen processors have strong multi-core performance. This directly addresses whether AMD supports multi-core, providing a clear affirmative answer to the question.\\nLABEL: \"relevant\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18aee44228d90a80</td>\n",
       "      <td>6027ca1f7c45cb40e05605d3b6bb73b5</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.380465</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18aee44228d90a80</td>\n",
       "      <td>6027ca1f7c45cb40e05605d3b6bb73b5</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>db538914-83d3-45b7-81e9-6cdd203b2c7c</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     context.span_id                  context.trace_id  \\\n",
       "0   99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
       "1   99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
       "2   99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
       "3   0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
       "4   0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
       "5   0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
       "6   c149e18e115a10db  012ba572ab94742d40164868224f2599   \n",
       "7   c149e18e115a10db  012ba572ab94742d40164868224f2599   \n",
       "8   c149e18e115a10db  012ba572ab94742d40164868224f2599   \n",
       "9   18aee44228d90a80  6027ca1f7c45cb40e05605d3b6bb73b5   \n",
       "10  18aee44228d90a80  6027ca1f7c45cb40e05605d3b6bb73b5   \n",
       "11  18aee44228d90a80  6027ca1f7c45cb40e05605d3b6bb73b5   \n",
       "\n",
       "                                        input  \\\n",
       "0   Can you give me any details about Samsung   \n",
       "1   Can you give me any details about Samsung   \n",
       "2   Can you give me any details about Samsung   \n",
       "3        I want to know information about AMD   \n",
       "4        I want to know information about AMD   \n",
       "5        I want to know information about AMD   \n",
       "6             I want to compare AMD and Intel   \n",
       "7             I want to compare AMD and Intel   \n",
       "8             I want to compare AMD and Intel   \n",
       "9                       Is AMD has multi core   \n",
       "10                      Is AMD has multi core   \n",
       "11                      Is AMD has multi core   \n",
       "\n",
       "                             document.id  document.score  \\\n",
       "0   09b3a531-26e4-4023-8b0a-ae6af3affc40        0.001939   \n",
       "1   a0149ea1-fb03-4f53-be3a-891702d05314        0.000306   \n",
       "2   bb1046e5-feb9-4bb8-8546-95829d996604        0.000189   \n",
       "3   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.025658   \n",
       "4   825177e1-6c77-4de7-bb02-c332e8dbaf54        0.007090   \n",
       "5   a23b9fae-a936-451e-b8d5-573ab30f08dd        0.004895   \n",
       "6   def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.857612   \n",
       "7   207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.754944   \n",
       "8   a23b9fae-a936-451e-b8d5-573ab30f08dd        0.735794   \n",
       "9   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.993957   \n",
       "10  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.380465   \n",
       "11  db538914-83d3-45b7-81e9-6cdd203b2c7c        0.027638   \n",
       "\n",
       "                                                                                                                            reference  \\\n",
       "0     the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per   \n",
       "1    its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of    \n",
       "2    dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"   \n",
       "3   \\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "4   nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "5   nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "6   ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi   \n",
       "7   s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong    \n",
       "8   nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "9   \\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "10  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "11   el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t   \n",
       "\n",
       "   eval_label  eval_score  \\\n",
       "0    relevant         1.0   \n",
       "1   unrelated         0.0   \n",
       "2    relevant         1.0   \n",
       "3    relevant         1.0   \n",
       "4    relevant         1.0   \n",
       "5    relevant         1.0   \n",
       "6    relevant         1.0   \n",
       "7    relevant         1.0   \n",
       "8    relevant         1.0   \n",
       "9    relevant         1.0   \n",
       "10       None         NaN   \n",
       "11       None         NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                               eval_explanation  \n",
       "0                           EXPLANATION: The question asks for details about Samsung. The reference text mentions \"flagship products\" related to Samsung, even though it does not provide specific details. However, it does indicate that Samsung's products have competed with Intel's in terms of price and performance. This information, while not detailed, does provide some context about Samsung's product range and market position.\\nLABEL: relevant  \n",
       "1                                                                                                                     EXPLANATION: The question asks for details about Samsung. The reference text does not mention Samsung at all, instead it discusses efficiency cores and hybrid architecture in the context of processor generations like Alder Lake and Raptor Lake. This information is not relevant to Samsung as a company or brand.\\nLABEL: unrelated  \n",
       "2   EXPLANATION: The question asks for details about Samsung. The reference text mentions Samsung's process of fabricating CPUs and GPUs using advanced process nodes and not owning its own fabs. This information provides insight into Samsung's manufacturing strategy, which can be considered relevant details about the company. However, the text does not provide a comprehensive overview of Samsung, so it is limited in scope.\\n\\nLABEL: \"relevant\"  \n",
       "3                                                                                                                                                      EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and provides specific information about their performance. This information can help answer the question by providing details about AMD's products and their capabilities.\\nLABEL: relevant  \n",
       "4                                                                                                                                                     EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant  \n",
       "5                                                                                                                                                                                                                      EXPLANATION: The question asks for information about AMD. The reference text mentions AMD specifically, noting its focus on Radeon graphics. This information is directly relevant to answering the question about AMD.\\nLABEL: relevant  \n",
       "6                              EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both AMD and Intel, discussing their performance and pricing. It also hints at their future focus areas. While the text does not provide a detailed comparison, it does mention both companies, which is a starting point for a comparison. Therefore, the reference text contains information relevant to the question.\\n\\nLABEL: relevant  \n",
       "7                                                                                                                                                           EXPLANATION: The reference text mentions both AMD and Intel, which are the companies the question is comparing. Although the text does not provide specific details for comparison, it does mention both companies, indicating that the reference text is related to the question.\\nLABEL: relevant  \n",
       "8                                                                                                            EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both companies and their expansion into GPU technologies, which is directly related to comparing their product lines and strategic directions. This information can help in understanding the competitive landscape between AMD and Intel.\\n\\nLABEL: relevant  \n",
       "9                                                                                                                                                              EXPLANATION: The question asks if AMD has multi-core capabilities. The reference text mentions that AMD's Ryzen processors have strong multi-core performance. This directly addresses whether AMD supports multi-core, providing a clear affirmative answer to the question.\\nLABEL: \"relevant\"  \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                         None  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                         None  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_with_relevance_df = pd.concat(\n",
    "    [retrieved_documents_df, retrieved_documents_relevance_df.add_prefix(\"eval_\")], axis=1\n",
    ")\n",
    "documents_with_relevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bfe8c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    context.span_id                  context.trace_id  \\\n",
      "0  99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
      "1  99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
      "2  99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
      "3  0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
      "4  0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
      "\n",
      "                                       input  \\\n",
      "0  Can you give me any details about Samsung   \n",
      "1  Can you give me any details about Samsung   \n",
      "2  Can you give me any details about Samsung   \n",
      "3       I want to know information about AMD   \n",
      "4       I want to know information about AMD   \n",
      "\n",
      "                            document.id  document_score  \\\n",
      "0  09b3a531-26e4-4023-8b0a-ae6af3affc40        0.001939   \n",
      "1  a0149ea1-fb03-4f53-be3a-891702d05314        0.000306   \n",
      "2  bb1046e5-feb9-4bb8-8546-95829d996604        0.000189   \n",
      "3  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.025658   \n",
      "4  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.007090   \n",
      "\n",
      "                                                                                                                           reference  \\\n",
      "0    the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per   \n",
      "1   its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of    \n",
      "2   dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"   \n",
      "3  \\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
      "4  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
      "\n",
      "       label  score  \\\n",
      "0   relevant    1.0   \n",
      "1  unrelated    0.0   \n",
      "2   relevant    1.0   \n",
      "3   relevant    1.0   \n",
      "4   relevant    1.0   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                   explanation  \n",
      "0                          EXPLANATION: The question asks for details about Samsung. The reference text mentions \"flagship products\" related to Samsung, even though it does not provide specific details. However, it does indicate that Samsung's products have competed with Intel's in terms of price and performance. This information, while not detailed, does provide some context about Samsung's product range and market position.\\nLABEL: relevant  \n",
      "1                                                                                                                    EXPLANATION: The question asks for details about Samsung. The reference text does not mention Samsung at all, instead it discusses efficiency cores and hybrid architecture in the context of processor generations like Alder Lake and Raptor Lake. This information is not relevant to Samsung as a company or brand.\\nLABEL: unrelated  \n",
      "2  EXPLANATION: The question asks for details about Samsung. The reference text mentions Samsung's process of fabricating CPUs and GPUs using advanced process nodes and not owning its own fabs. This information provides insight into Samsung's manufacturing strategy, which can be considered relevant details about the company. However, the text does not provide a comprehensive overview of Samsung, so it is limited in scope.\\n\\nLABEL: \"relevant\"  \n",
      "3                                                                                                                                                     EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and provides specific information about their performance. This information can help answer the question by providing details about AMD's products and their capabilities.\\nLABEL: relevant  \n",
      "4                                                                                                                                                    EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant  \n"
     ]
    }
   ],
   "source": [
    "documents_with_relevance_df = documents_with_relevance_df.rename(\n",
    "    columns={\n",
    "        \"eval_label\": \"label\",\n",
    "        \"eval_score\": \"score\",\n",
    "        \"eval_explanation\": \"explanation\",\n",
    "        \"document.score\": \"document_score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(documents_with_relevance_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c527b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>document.id</th>\n",
       "      <th>document_score</th>\n",
       "      <th>reference</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>79a556783429c63d0be2f78cc668e5b2</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>09b3a531-26e4-4023-8b0a-ae6af3affc40</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for details about Samsung. The reference text mentions \"flagship products\" related to Samsung, even though it does not provide specific details. However, it does indicate that Samsung's products have competed with Intel's in terms of price and performance. This information, while not detailed, does provide some context about Samsung's product range and market position.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>79a556783429c63d0be2f78cc668e5b2</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EXPLANATION: The question asks for details about Samsung. The reference text does not mention Samsung at all, instead it discusses efficiency cores and hybrid architecture in the context of processor generations like Alder Lake and Raptor Lake. This information is not relevant to Samsung as a company or brand.\\nLABEL: unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>79a556783429c63d0be2f78cc668e5b2</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>bb1046e5-feb9-4bb8-8546-95829d996604</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for details about Samsung. The reference text mentions Samsung's process of fabricating CPUs and GPUs using advanced process nodes and not owning its own fabs. This information provides insight into Samsung's manufacturing strategy, which can be considered relevant details about the company. However, the text does not provide a comprehensive overview of Samsung, so it is limited in scope.\\n\\nLABEL: \"relevant\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>8c37e9fe85f8bf62b84186e36a95eece</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.025658</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and provides specific information about their performance. This information can help answer the question by providing details about AMD's products and their capabilities.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>8c37e9fe85f8bf62b84186e36a95eece</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>8c37e9fe85f8bf62b84186e36a95eece</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information about AMD. The reference text mentions AMD specifically, noting its focus on Radeon graphics. This information is directly relevant to answering the question about AMD.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c149e18e115a10db</td>\n",
       "      <td>012ba572ab94742d40164868224f2599</td>\n",
       "      <td>I want to compare AMD and Intel</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.857612</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both AMD and Intel, discussing their performance and pricing. It also hints at their future focus areas. While the text does not provide a detailed comparison, it does mention both companies, which is a starting point for a comparison. Therefore, the reference text contains information relevant to the question.\\n\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>c149e18e115a10db</td>\n",
       "      <td>012ba572ab94742d40164868224f2599</td>\n",
       "      <td>I want to compare AMD and Intel</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.754944</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The reference text mentions both AMD and Intel, which are the companies the question is comparing. Although the text does not provide specific details for comparison, it does mention both companies, indicating that the reference text is related to the question.\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c149e18e115a10db</td>\n",
       "      <td>012ba572ab94742d40164868224f2599</td>\n",
       "      <td>I want to compare AMD and Intel</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.735794</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both companies and their expansion into GPU technologies, which is directly related to comparing their product lines and strategic directions. This information can help in understanding the competitive landscape between AMD and Intel.\\n\\nLABEL: relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18aee44228d90a80</td>\n",
       "      <td>6027ca1f7c45cb40e05605d3b6bb73b5</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.993957</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks if AMD has multi-core capabilities. The reference text mentions that AMD's Ryzen processors have strong multi-core performance. This directly addresses whether AMD supports multi-core, providing a clear affirmative answer to the question.\\nLABEL: \"relevant\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18aee44228d90a80</td>\n",
       "      <td>6027ca1f7c45cb40e05605d3b6bb73b5</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.380465</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18aee44228d90a80</td>\n",
       "      <td>6027ca1f7c45cb40e05605d3b6bb73b5</td>\n",
       "      <td>Is AMD has multi core</td>\n",
       "      <td>db538914-83d3-45b7-81e9-6cdd203b2c7c</td>\n",
       "      <td>0.027638</td>\n",
       "      <td>el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     context.span_id                  context.trace_id  \\\n",
       "0   99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
       "1   99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
       "2   99ada09c725d14af  79a556783429c63d0be2f78cc668e5b2   \n",
       "3   0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
       "4   0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
       "5   0a46c739623afaf8  8c37e9fe85f8bf62b84186e36a95eece   \n",
       "6   c149e18e115a10db  012ba572ab94742d40164868224f2599   \n",
       "7   c149e18e115a10db  012ba572ab94742d40164868224f2599   \n",
       "8   c149e18e115a10db  012ba572ab94742d40164868224f2599   \n",
       "9   18aee44228d90a80  6027ca1f7c45cb40e05605d3b6bb73b5   \n",
       "10  18aee44228d90a80  6027ca1f7c45cb40e05605d3b6bb73b5   \n",
       "11  18aee44228d90a80  6027ca1f7c45cb40e05605d3b6bb73b5   \n",
       "\n",
       "                                        input  \\\n",
       "0   Can you give me any details about Samsung   \n",
       "1   Can you give me any details about Samsung   \n",
       "2   Can you give me any details about Samsung   \n",
       "3        I want to know information about AMD   \n",
       "4        I want to know information about AMD   \n",
       "5        I want to know information about AMD   \n",
       "6             I want to compare AMD and Intel   \n",
       "7             I want to compare AMD and Intel   \n",
       "8             I want to compare AMD and Intel   \n",
       "9                       Is AMD has multi core   \n",
       "10                      Is AMD has multi core   \n",
       "11                      Is AMD has multi core   \n",
       "\n",
       "                             document.id  document_score  \\\n",
       "0   09b3a531-26e4-4023-8b0a-ae6af3affc40        0.001939   \n",
       "1   a0149ea1-fb03-4f53-be3a-891702d05314        0.000306   \n",
       "2   bb1046e5-feb9-4bb8-8546-95829d996604        0.000189   \n",
       "3   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.025658   \n",
       "4   825177e1-6c77-4de7-bb02-c332e8dbaf54        0.007090   \n",
       "5   a23b9fae-a936-451e-b8d5-573ab30f08dd        0.004895   \n",
       "6   def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.857612   \n",
       "7   207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.754944   \n",
       "8   a23b9fae-a936-451e-b8d5-573ab30f08dd        0.735794   \n",
       "9   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.993957   \n",
       "10  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.380465   \n",
       "11  db538914-83d3-45b7-81e9-6cdd203b2c7c        0.027638   \n",
       "\n",
       "                                                                                                                            reference  \\\n",
       "0     the more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per   \n",
       "1    its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of    \n",
       "2    dvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"   \n",
       "3   \\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "4   nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "5   nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "6   ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi   \n",
       "7   s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong    \n",
       "8   nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "9   \\n    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "10  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "11   el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t   \n",
       "\n",
       "        label  score  \\\n",
       "0    relevant    1.0   \n",
       "1   unrelated    0.0   \n",
       "2    relevant    1.0   \n",
       "3    relevant    1.0   \n",
       "4    relevant    1.0   \n",
       "5    relevant    1.0   \n",
       "6    relevant    1.0   \n",
       "7    relevant    1.0   \n",
       "8    relevant    1.0   \n",
       "9    relevant    1.0   \n",
       "10       None    NaN   \n",
       "11       None    NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                    explanation  \n",
       "0                           EXPLANATION: The question asks for details about Samsung. The reference text mentions \"flagship products\" related to Samsung, even though it does not provide specific details. However, it does indicate that Samsung's products have competed with Intel's in terms of price and performance. This information, while not detailed, does provide some context about Samsung's product range and market position.\\nLABEL: relevant  \n",
       "1                                                                                                                     EXPLANATION: The question asks for details about Samsung. The reference text does not mention Samsung at all, instead it discusses efficiency cores and hybrid architecture in the context of processor generations like Alder Lake and Raptor Lake. This information is not relevant to Samsung as a company or brand.\\nLABEL: unrelated  \n",
       "2   EXPLANATION: The question asks for details about Samsung. The reference text mentions Samsung's process of fabricating CPUs and GPUs using advanced process nodes and not owning its own fabs. This information provides insight into Samsung's manufacturing strategy, which can be considered relevant details about the company. However, the text does not provide a comprehensive overview of Samsung, so it is limited in scope.\\n\\nLABEL: \"relevant\"  \n",
       "3                                                                                                                                                      EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and provides specific information about their performance. This information can help answer the question by providing details about AMD's products and their capabilities.\\nLABEL: relevant  \n",
       "4                                                                                                                                                     EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant  \n",
       "5                                                                                                                                                                                                                      EXPLANATION: The question asks for information about AMD. The reference text mentions AMD specifically, noting its focus on Radeon graphics. This information is directly relevant to answering the question about AMD.\\nLABEL: relevant  \n",
       "6                              EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both AMD and Intel, discussing their performance and pricing. It also hints at their future focus areas. While the text does not provide a detailed comparison, it does mention both companies, which is a starting point for a comparison. Therefore, the reference text contains information relevant to the question.\\n\\nLABEL: relevant  \n",
       "7                                                                                                                                                           EXPLANATION: The reference text mentions both AMD and Intel, which are the companies the question is comparing. Although the text does not provide specific details for comparison, it does mention both companies, indicating that the reference text is related to the question.\\nLABEL: relevant  \n",
       "8                                                                                                            EXPLANATION: The question asks to compare AMD and Intel. The reference text mentions both companies and their expansion into GPU technologies, which is directly related to comparing their product lines and strategic directions. This information can help in understanding the competitive landscape between AMD and Intel.\\n\\nLABEL: relevant  \n",
       "9                                                                                                                                                              EXPLANATION: The question asks if AMD has multi-core capabilities. The reference text mentions that AMD's Ryzen processors have strong multi-core performance. This directly addresses whether AMD supports multi-core, providing a clear affirmative answer to the question.\\nLABEL: \"relevant\"  \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                         None  \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                         None  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_with_relevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05568b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_df = documents_with_relevance_df.copy()\n",
    "\n",
    "# √©p score v·ªÅ s·ªë, bi·∫øn l·ªói th√†nh NaN\n",
    "final_df['__score__'] = pd.to_numeric(final_df['document_score'], errors='coerce')\n",
    "\n",
    "# sort theo span_id v√† score gi·∫£m d·∫ßn; NaN ƒë·∫©y xu·ªëng cu·ªëi nh√≥m\n",
    "final_df = final_df.sort_values(\n",
    "    ['context.span_id', '__score__'], ascending=[True, False]\n",
    ")\n",
    "\n",
    "# ƒë√°nh s·ªë 0,1,2,... trong t·ª´ng span\n",
    "final_df['document_position'] = final_df.groupby('context.span_id').cumcount()\n",
    "\n",
    "# MultiIndex ƒë√∫ng chu·∫©n Phoenix (n·∫øu b·∫°n c·∫ßn)\n",
    "final_df = final_df.set_index(['context.span_id', 'document_position']).drop(columns='__score__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31bddd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context.trace_id', 'input', 'document.id', 'document_score',\n",
       "       'reference', 'label', 'score', 'explanation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f841a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "def _compute_ndcg(df: pd.DataFrame, k: int):\n",
    "    \"\"\"Compute NDCG@k in the presence of missing values\"\"\"\n",
    "    n = max(2, len(df))\n",
    "    eval_scores = np.zeros(n)\n",
    "    doc_scores = np.zeros(n)\n",
    "    eval_scores[: len(df)] = df.score\n",
    "    doc_scores[: len(df)] = df.document_score\n",
    "    try:\n",
    "        return ndcg_score([eval_scores], [doc_scores], k=k)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "ndcg_at_2 = pd.DataFrame(\n",
    "    {\"score\": final_df.groupby(\"context.span_id\").apply(_compute_ndcg, k=2)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56f60504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0a46c739623afaf8</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18aee44228d90a80</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99ada09c725d14af</th>\n",
       "      <td>0.613147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c149e18e115a10db</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     score\n",
       "context.span_id           \n",
       "0a46c739623afaf8  1.000000\n",
       "18aee44228d90a80       NaN\n",
       "99ada09c725d14af  0.613147\n",
       "c149e18e115a10db  1.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38831399",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_2 = pd.DataFrame(\n",
    "    {\n",
    "        \"score\": final_df.groupby(\"context.span_id\").apply(\n",
    "            lambda x: x.score[:2].sum(skipna=False) / 2\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8339330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0a46c739623afaf8</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18aee44228d90a80</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99ada09c725d14af</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c149e18e115a10db</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score\n",
       "context.span_id        \n",
       "0a46c739623afaf8    1.0\n",
       "18aee44228d90a80    NaN\n",
       "99ada09c725d14af    0.5\n",
       "c149e18e115a10db    1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "59a0a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = pd.DataFrame(\n",
    "    {\n",
    "        \"hit\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n",
    "            lambda x: x.score[:2].sum(skipna=False) > 0\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb5a69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(dataframe=ndcg_at_2, eval_name=\"ndcg@2\"),\n",
    "    SpanEvaluations(dataframe=precision_at_2, eval_name=\"precision@2\"),\n",
    "    DocumentEvaluations(dataframe=final_df, eval_name=\"relevance\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1a6e2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 spans to evaluate\n",
      "                 context.span.id context.trace.id            input.value  \\\n",
      "context.span_id                                                            \n",
      "18aee44228d90a80            None             None  Is AMD has multi core   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          retrieval.documents  \n",
      "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "18aee44228d90a80  [{'document': {'id': '6e13a8d5-0deb-4982-8f6a-601164f66a73', 'score': 0.9939568673, 'content': '\n",
      "    \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat'}}, {'document': {'id': '825177e1-6c77-4de7-bb02-c332e8dbaf54', 'score': 0.38046522790000004, 'content': 'nerations. \"\n",
      "    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b'}}, {'document': {'id': 'db538914-83d3-45b7-81e9-6cdd203b2c7c', 'score': 0.027637798800000002, 'content': 'el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t'}}]  \n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "\n",
    "query = (\n",
    "        SpanQuery()\n",
    "        .where(\"span_kind == 'RETRIEVER' and evals['ndcg@2'].score is None\")\n",
    "        .select(\"context.span.id\", \"context.trace.id\", \"input.value\", \"retrieval.documents\")\n",
    "    )\n",
    "\n",
    "df = px.Client().query_spans(query, project_name=\"hugging-face\")\n",
    "print(f\"Found {len(df)} spans to evaluate\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9e99fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents CH∆ØA evaluate: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>0</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>the more affordable choice, but in recent yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>1</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99ada09c725d14af</td>\n",
       "      <td>2</td>\n",
       "      <td>Can you give me any details about Samsung</td>\n",
       "      <td>dvanced process nodes to fabricate its CPUs an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>0</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>1</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0a46c739623afaf8</td>\n",
       "      <td>2</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            span_id  document_position  \\\n",
       "0  99ada09c725d14af                  0   \n",
       "1  99ada09c725d14af                  1   \n",
       "2  99ada09c725d14af                  2   \n",
       "3  0a46c739623afaf8                  0   \n",
       "4  0a46c739623afaf8                  1   \n",
       "5  0a46c739623afaf8                  2   \n",
       "\n",
       "                                       input  \\\n",
       "0  Can you give me any details about Samsung   \n",
       "1  Can you give me any details about Samsung   \n",
       "2  Can you give me any details about Samsung   \n",
       "3       I want to know information about AMD   \n",
       "4       I want to know information about AMD   \n",
       "5       I want to know information about AMD   \n",
       "\n",
       "                                           reference  \n",
       "0   the more affordable choice, but in recent yea...  \n",
       "1   its efficiency cores and hybrid architecture,...  \n",
       "2  dvanced process nodes to fabricate its CPUs an...  \n",
       "3  \\n    \"AMD's Ryzen processors have been praise...  \n",
       "4  nerations. \"\\n    \"The launch of AMD's EPYC se...  \n",
       "5  nd pre-built PCs. \"\\n    \"Both companies are e...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "PROJECT = \"hugging-face\"\n",
    "client = px.Client()\n",
    "\n",
    "# 1) Explode retrieved documents -> m·ªói d√≤ng = 1 doc\n",
    "q_docs = (\n",
    "    SpanQuery()\n",
    "    .where(\"span_kind == 'RETRIEVER'\")\n",
    "    .select(input=\"input.value\")\n",
    "    .explode(\"retrieval.documents\", reference=\"document.content\")\n",
    ")\n",
    "docs = client.query_spans(q_docs, project_name=PROJECT)\n",
    "\n",
    "# ƒëem 2 c·∫•p index ra c·ªôt th∆∞·ªùng ƒë·ªÉ join\n",
    "docs = docs.reset_index()  # c√≥ c·ªôt context.span_id, document_position\n",
    "docs = docs.rename(columns={\"context.span_id\": \"span_id\"})\n",
    "\n",
    "# 2) L·∫•y t·∫•t c·∫£ DocumentEvaluations t√™n 'relevance' ƒë√£ log (span_id + document_position)\n",
    "ev_list = client.get_evaluations(project_name=PROJECT)  # tr·∫£ v·ªÅ list Evaluations\n",
    "ev_parts = []\n",
    "for ev in ev_list:\n",
    "    df = getattr(ev, \"dataframe\", None)\n",
    "    if df is None or ev.eval_name != \"relevance\":\n",
    "        continue\n",
    "    df = df.reset_index()  # l√¥i span_id, document_position ra c·ªôt\n",
    "    if {\"span_id\", \"document_position\"}.issubset(df.columns):\n",
    "        ev_parts.append(df[[\"span_id\", \"document_position\"]])\n",
    "\n",
    "evaluated = (\n",
    "    pd.concat(ev_parts, ignore_index=True)\n",
    "    if ev_parts else pd.DataFrame(columns=[\"span_id\", \"document_position\"])\n",
    ")\n",
    "\n",
    "# 3) Anti-join -> ch·ªâ gi·ªØ documents CH∆ØA ƒë∆∞·ª£c evaluate\n",
    "unevaluated = (\n",
    "    docs.merge(evaluated, on=[\"span_id\", \"document_position\"], how=\"left\", indicator=True)\n",
    "        .query(\"_merge == 'left_only'\")\n",
    "        .drop(columns=[\"_merge\"])\n",
    ")\n",
    "\n",
    "print(f\"Documents CH∆ØA evaluate: {len(unevaluated)}\")\n",
    "unevaluated\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiteLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
