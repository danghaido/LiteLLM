{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350584fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "from LiteLLM.common import CONFIG\n",
    "\n",
    "from phoenix.evals import (\n",
    "    llm_classify,\n",
    "    LiteLLMModel,\n",
    "    llm_generate,\n",
    "    RelevanceEvaluator,\n",
    "    run_evals\n",
    ")\n",
    "\n",
    "import litellm\n",
    "litellm._turn_on_debug()\n",
    "\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\"\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = CONFIG.api_key\n",
    "\n",
    "import phoenix as px\n",
    "import pandas as pd\n",
    "\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "from phoenix.trace import SpanEvaluations, using_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dfca108",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "def normalize_newline(s: str) -> str:\n",
    "    \"\"\"Chuy·ªÉn '\\\\n' literal th√†nh newline th·∫≠t.\"\"\"\n",
    "    return s.replace(\"\\\\n\", \"\\n\") if isinstance(s, str) else s\n",
    "\n",
    "def split_ref_items(s: str):\n",
    "    \"\"\"\n",
    "    T√°ch chu·ªói ref d·∫°ng:\n",
    "    [1] Intel focuses...\n",
    "    [2] AMD Ryzen offers...\n",
    "    [4] Laptop battery life...\n",
    "    Th√†nh list [(id, text), ...]\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = normalize_newline(s)\n",
    "    pattern = re.compile(r\"\\[(\\d+)\\]\\s*(.*?)(?=(?:\\n\\[\\d+\\])|$)\", flags=re.S)\n",
    "    return pattern.findall(s)\n",
    "\n",
    "def explode_refs(df: pd.DataFrame, ref_col: str = \"ref\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Nh·∫≠n DataFrame c√≥ c·ªôt 'ref', tr·∫£ v·ªÅ DataFrame ch·ªâ g·ªìm context.trace_id v√† reference.\n",
    "    \"\"\"\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ref_items\"] = tmp[ref_col].apply(split_ref_items)\n",
    "    out = tmp.explode(\"ref_items\", ignore_index=True)\n",
    "    out[[\"ref_id\", \"reference\"]] = pd.DataFrame(out[\"ref_items\"].tolist(), index=out.index)\n",
    "    out[\"ref_id\"] = out[\"ref_id\"].astype(int)\n",
    "    return out[[\"reference\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e8e511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Exploded format:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                        reference\n",
      "0  T·ªïng Quan V·ªÅ Th·∫ø Gi·ªõi M√°y T√≠nh: T·ª´ Ph·∫ßn C·ª©ng ƒê·∫øn An Ninh M·∫°ng\\nM√°y t√≠nh ƒë√£ tr·ªü th√†nh m·ªôt c√¥ng c·ª• kh√¥ng th·ªÉ thi·∫øu trong cu·ªôc s·ªëng hi·ªán ƒë·∫°i, t·ª´ c√¥ng vi·ªác, h·ªçc t·∫≠p ƒë·∫øn gi·∫£i tr√≠. ƒê·ªÉ hi·ªÉu r√µ c√°ch th·ª©c ho·∫°t ƒë·ªông c·ªßa m·ªôt c·ªó m√°y ph·ª©c t·∫°p n√†y, ch√∫ng ta c·∫ßn t√¨m hi·ªÉu hai th√†nh ph·∫ßn c·ªët l√µi c·∫•u t·∫°o n√™n n√≥: ph·∫ßn c·ª©ng (hardware) v√† ph·∫ßn m·ªÅm (software). N·∫øu ph·∫ßn c·ª©ng l√† th·ªÉ x√°c h·ªØu h√¨nh, l√† nh·ªØng linh ki·ªán v·∫≠\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "query = SpanQuery().where(\"span_kind == 'CHAIN'\", ).select(\"trace_id\", input=\"input.value\", output=\"output.value\")\n",
    "\n",
    "df = px.Client().query_spans(query, project_name=\"hugging-face\")\n",
    "\n",
    "reference = SpanQuery().where(\"span_kind == 'TOOL'\").select(\"trace_id\", ref=\"prompt.context.preview\")\n",
    "spans_with_docs_df = px.Client().query_spans(reference, project_name=\"hugging-face\")\n",
    "print(len(spans_with_docs_df))\n",
    "\n",
    "document_chunks_df = explode_refs(spans_with_docs_df)\n",
    "print(\"Exploded format:\\n\", document_chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b192a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_questions_template = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{ref_text}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "3 questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided.\"\n",
    "\n",
    "Output the questions in JSON format with the keys question_1, question_2, question_3.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d13026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_newline(s: str) -> str:\n",
    "    \"\"\"Chuy·ªÉn '\\\\n' literal th√†nh newline th·∫≠t.\"\"\"\n",
    "    return s.replace(\"\\\\n\", \"\\n\") if isinstance(s, str) else s\n",
    "\n",
    "def split_ref_items(s: str):\n",
    "    \"\"\"\n",
    "    T√°ch chu·ªói ref d·∫°ng:\n",
    "    [1] Intel focuses...\n",
    "    [2] AMD Ryzen offers...\n",
    "    [4] Laptop battery life...\n",
    "    Th√†nh list [(id, text), ...]\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    s = normalize_newline(s)\n",
    "    pattern = re.compile(r\"\\[(\\d+)\\]\\s*(.*?)(?=(?:\\n\\[\\d+\\])|$)\", flags=re.S)\n",
    "    return pattern.findall(s)\n",
    "\n",
    "def explode_refs(df: pd.DataFrame, ref_col: str = \"ref\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Nh·∫≠n DataFrame c√≥ c·ªôt 'ref', tr·∫£ v·ªÅ DataFrame ch·ªâ g·ªìm context.trace_id v√† ref_text.\n",
    "    \"\"\"\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ref_items\"] = tmp[ref_col].apply(split_ref_items)\n",
    "    out = tmp.explode(\"ref_items\", ignore_index=True)\n",
    "    out[[\"ref_id\", \"ref_text\"]] = pd.DataFrame(out[\"ref_items\"].tolist(), index=out.index)\n",
    "    out[\"ref_id\"] = out[\"ref_id\"].astype(int)\n",
    "    return out[[\"context.trace_id\", \"ref_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ae3e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_output_parser(response: str, index: int):\n",
    "    s = (response or \"\").strip()\n",
    "    # log v√†i m·∫´u ƒë·∫ßu ƒë·ªÉ b·∫°n xem raw\n",
    "    if index < 3:\n",
    "        print(f\"\\nRAW[{index}]:\\n{repr(s)}\\n\")\n",
    "\n",
    "    if not s:\n",
    "        return {\"__error__\": \"empty\", \"question_1\": None, \"question_2\": None, \"question_3\": None}\n",
    "\n",
    "    # 1) b·ªè code fences ```json ... ```\n",
    "    if s.startswith(\"```\"):\n",
    "        s = re.sub(r\"^```[a-zA-Z]*\\s*\", \"\", s)\n",
    "        s = re.sub(r\"\\s*```$\", \"\", s, flags=re.S)\n",
    "\n",
    "    # 2) l·∫•y block JSON ƒë·∫ßu ti√™n\n",
    "    start, end = s.find(\"{\"), s.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        try:\n",
    "            obj = json.loads(s[start:end+1])\n",
    "            # ƒë·∫£m b·∫£o tr·∫£ v·ªÅ dict c√≥ c√°c key mong mu·ªën\n",
    "            return {\n",
    "                \"question_1\": obj.get(\"question_1\"),\n",
    "                \"question_2\": obj.get(\"question_2\"),\n",
    "                \"question_3\": obj.get(\"question_3\"),\n",
    "            }\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass\n",
    "\n",
    "    # 3) fallback: n·∫øu model tr·∫£ plain text, nh√©t v√†o question_1\n",
    "    return {\"question_1\": s, \"question_2\": None, \"question_3\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81ef4859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n",
      "llm_generate |          | 0/6 (0.0%) | ‚è≥ 00:00<? | ?it/s\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': 'Context information is below.\\n\\n---------------------\\nthe more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:26:59 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\nthe more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:26:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\nthe more affordable choice, but in recent years, some of its flagship products even surpassed Intel‚Äôs in both price and raw per\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRMue-3NKUce-97432f3f884c99fb\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042019,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 6190086200822693000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"```json\\n{\\n  \\\"question_1\\\": \\\"According to the context, which company's flagship products have recently surpassed Intel's in terms of both price and performance?\\\",\\n  \\\"question_2\\\": \\\"What does the phrase 'the more affordable choice' likely refer to in the given context?\\\",\\n  \\\"question_3\\\": \\\"Based on the information provided, what trend can be observed in the competitive landscape between the two companies mentioned?\\\"\\n}\\n```\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 146,\n",
      "    \"completion_tokens\": 90,\n",
      "    \"total_tokens\": 236,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:27:00 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "llm_generate |‚ñà‚ñã        | 1/6 (16.7%) | ‚è≥ 00:01<00:08 |  1.73s/it\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': 'Context information is below.\\n\\n---------------------\\nits efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:27:00 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\nits efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW[0]:\n",
      "'```json\\n{\\n  \"question_1\": \"According to the context, which company\\'s flagship products have recently surpassed Intel\\'s in terms of both price and performance?\",\\n  \"question_2\": \"What does the phrase \\'the more affordable choice\\' likely refer to in the given context?\",\\n  \"question_3\": \"Based on the information provided, what trend can be observed in the competitive landscape between the two companies mentioned?\"\\n}\\n```'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:27:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\nits efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRNSG-3NKUce-97432f4a39f4200c\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042021,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 11450438168943262000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"```json\\n{\\n  \\\"question_1\\\": \\\"What are the two specific processor generations mentioned in the context that feature efficiency cores and a hybrid architecture?\\\",\\n  \\\"question_2\\\": \\\"Identify the term used in the context to describe the combination of different types of processor cores in these generations.\\\",\\n  \\\"question_3\\\": \\\"Based on the context, which event is indicated to have occurred with the mention of 'The launch of'?\\\"\\n}\\n```\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 146,\n",
      "    \"completion_tokens\": 92,\n",
      "    \"total_tokens\": 238,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:27:02 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñé      | 2/6 (33.3%) | ‚è≥ 00:03<00:07 |  1.80s/it\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': 'Context information is below.\\n\\n---------------------\\ndvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:27:02 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\ndvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW[1]:\n",
      "'```json\\n{\\n  \"question_1\": \"What are the two specific processor generations mentioned in the context that feature efficiency cores and a hybrid architecture?\",\\n  \"question_2\": \"Identify the term used in the context to describe the combination of different types of processor cores in these generations.\",\\n  \"question_3\": \"Based on the context, which event is indicated to have occurred with the mention of \\'The launch of\\'?\"\\n}\\n```'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:27:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\ndvanced process nodes to fabricate its CPUs and GPUs, allowing it to deliver competitive products without owning its own fabs. \"\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRNth-3NKUce-97432f552ff6200c\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042023,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 901961998825556100,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"```json\\n{\\n  \\\"question_1\\\": \\\"What does the term 'advanced process nodes' refer to in the context of CPU and GPU fabrication?\\\",\\n  \\\"question_2\\\": \\\"Why might a company choose not to own its own fabrication plants (fabs) when producing CPUs and GPUs?\\\",\\n  \\\"question_3\\\": \\\"How does using advanced process nodes contribute to delivering competitive products in the CPU and GPU market?\\\"\\n}\\n```\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 145,\n",
      "    \"completion_tokens\": 87,\n",
      "    \"total_tokens\": 232,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:27:04 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 (50.0%) | ‚è≥ 00:05<00:05 |  1.81s/it\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': 'Context information is below.\\n\\n---------------------\\n\"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m20:27:04 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\n\"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW[2]:\n",
      "'```json\\n{\\n  \"question_1\": \"What does the term \\'advanced process nodes\\' refer to in the context of CPU and GPU fabrication?\",\\n  \"question_2\": \"Why might a company choose not to own its own fabrication plants (fabs) when producing CPUs and GPUs?\",\\n  \"question_3\": \"How does using advanced process nodes contribute to delivering competitive products in the CPU and GPU market?\"\\n}\\n```'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m20:27:04 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****Hh' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': 'Context information is below.\\n\\n---------------------\\n\"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n---------------------\\n\\nGiven the context information and not prior knowledge.\\ngenerate only questions based on the below query.\\n\\nYou are a Teacher/ Professor. Your task is to setup 3 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\\n\\nOutput the questions in JSON format with the keys question_1, question_2, question_3.\\n', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m20:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8SRPVQ-3NKUce-97432f6079e20a89\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756042025,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 2426906414981683000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"```json\\n{\\n  \\\"question_1\\\": \\\"What feature of AMD's Ryzen processors has contributed to their popularity among content creators?\\\",\\n  \\\"question_2\\\": \\\"How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?\\\",\\n  \\\"question_3\\\": \\\"Based on the context, which type of users are likely to find AMD's Ryzen processors particularly appealing?\\\"\\n}\\n```\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 142,\n",
      "    \"completion_tokens\": 80,\n",
      "    \"total_tokens\": 222,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m20:27:05 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m20:27:05 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m20:27:05 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 (50.0%) | ‚è≥ 00:07<00:05 |  1.81s/it \u001b[92m20:27:06 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m20:27:06 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 (66.7%) | ‚è≥ 00:07<00:03 |  1.80s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process was interrupted. The return value will be incomplete...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = LiteLLMModel(\n",
    "    model=\"huggingface/together/Qwen/Qwen2.5-7B-Instruct\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "questions_df = llm_generate(\n",
    "    dataframe=document_chunks_df,\n",
    "    template=generate_questions_template,\n",
    "    model=model,\n",
    "    output_parser=robust_output_parser,\n",
    "    concurrency=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a45d86f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>question_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What feature of AMD's Ryzen processors has contributed to their popularity among content creators?</td>\n",
       "      <td>How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?</td>\n",
       "      <td>Based on the context, which type of users are likely to find AMD's Ryzen processors particularly appealing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What was the impact of AMD's EPYC server processors on the data center market according to the given context?</td>\n",
       "      <td>Based on the information provided, what specific feature of AMD's EPYC processors is mentioned as a key factor in disrupting the market?</td>\n",
       "      <td>Summarize the main point of the given context in one sentence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two companies mentioned in the context that are expanding into GPU technologies?</td>\n",
       "      <td>Which company is focusing on Radeon graphics according to the context?</td>\n",
       "      <td>What is the focus of Intel's expansion into GPU technologies based on the given information?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the context, what are both AMD and Intel focusing on in the future?</td>\n",
       "      <td>How does the context describe the current state of price and raw performance between AMD and Intel?</td>\n",
       "      <td>Summarize the main points mentioned about AMD and Intel's future strategies in the given context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the term 'fabs' refer to in the context of semiconductor manufacturing, and why might a company like AMD not own its own fabs?</td>\n",
       "      <td>How has AMD's performance in the desktop market changed in recent years, and what does the context suggest about Intel's position in this market?</td>\n",
       "      <td>Based on the given context, what can be inferred about the competitive landscape between AMD and Intel in the desktop market?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                 question_1  \\\n",
       "0                                        What feature of AMD's Ryzen processors has contributed to their popularity among content creators?   \n",
       "1                             What was the impact of AMD's EPYC server processors on the data center market according to the given context?   \n",
       "2                                             What are the two companies mentioned in the context that are expanding into GPU technologies?   \n",
       "3                                                          According to the context, what are both AMD and Intel focusing on in the future?   \n",
       "4  What does the term 'fabs' refer to in the context of semiconductor manufacturing, and why might a company like AMD not own its own fabs?   \n",
       "\n",
       "                                                                                                                                          question_2  \\\n",
       "0                                                      How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?   \n",
       "1           Based on the information provided, what specific feature of AMD's EPYC processors is mentioned as a key factor in disrupting the market?   \n",
       "2                                                                             Which company is focusing on Radeon graphics according to the context?   \n",
       "3                                                How does the context describe the current state of price and raw performance between AMD and Intel?   \n",
       "4  How has AMD's performance in the desktop market changed in recent years, and what does the context suggest about Intel's position in this market?   \n",
       "\n",
       "                                                                                                                      question_3  \n",
       "0                    Based on the context, which type of users are likely to find AMD's Ryzen processors particularly appealing?  \n",
       "1                                                                 Summarize the main point of the given context in one sentence.  \n",
       "2                                   What is the focus of Intel's expansion into GPU technologies based on the given information?  \n",
       "3                              Summarize the main points mentioned about AMD and Intel's future strategies in the given context.  \n",
       "4  Based on the given context, what can be inferred about the competitive landscape between AMD and Intel in the desktop market?  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4b730d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataframe of the questions and the document chunks\n",
    "questions_with_document_chunk_df = pd.concat([questions_df, document_chunks_df], axis=1)\n",
    "questions_with_document_chunk_df = questions_with_document_chunk_df.melt(\n",
    "    id_vars=[\"ref_text\"], value_name=\"question\"\n",
    ").drop(\"variable\", axis=1)\n",
    "# If the above step was interrupted, there might be questions missing. Let's run this to clean up the dataframe.\n",
    "questions_with_document_chunk_df = questions_with_document_chunk_df[\n",
    "    questions_with_document_chunk_df[\"question\"].notnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a958888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref_text</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>What feature of AMD's Ryzen processors has contributed to their popularity among content creators?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>What was the impact of AMD's EPYC server processors on the data center market according to the given context?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>What are the two companies mentioned in the context that are expanding into GPU technologies?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi</td>\n",
       "      <td>According to the context, what are both AMD and Intel focusing on in the future?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong</td>\n",
       "      <td>What does the term 'fabs' refer to in the context of semiconductor manufacturing, and why might a company like AMD not own its own fabs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun</td>\n",
       "      <td>What are the two companies mentioned in the context that are expanding into GPU technologies?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>What feature of AMD's Ryzen processors has contributed to their popularity among content creators?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b</td>\n",
       "      <td>What was the impact of AMD's EPYC server processors on the data center market according to the given context?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t</td>\n",
       "      <td>What challenge has el faced in adopting newer nanometer processes?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat</td>\n",
       "      <td>How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                            ref_text  \\\n",
       "0        \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "1  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "2  nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "3  ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi   \n",
       "4  s without owning its own fabs. \"\\n    \"While AMD gained significant momentum in the desktop market, Intel still maintains strong    \n",
       "5  nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun   \n",
       "6        \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "7  nerations. \"\\n    \"The launch of AMD's EPYC server processors disrupted the data center market, offering higher core counts and b   \n",
       "8   el has faced challenges with delays in adopting smaller nanometer processes, while AMD leveraged TSMC‚Äôs 7nm and 5nm technology t   \n",
       "9        \"AMD's Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat   \n",
       "\n",
       "                                                                                                                                   question  \n",
       "0                                        What feature of AMD's Ryzen processors has contributed to their popularity among content creators?  \n",
       "1                             What was the impact of AMD's EPYC server processors on the data center market according to the given context?  \n",
       "2                                             What are the two companies mentioned in the context that are expanding into GPU technologies?  \n",
       "3                                                          According to the context, what are both AMD and Intel focusing on in the future?  \n",
       "4  What does the term 'fabs' refer to in the context of semiconductor manufacturing, and why might a company like AMD not own its own fabs?  \n",
       "5                                             What are the two companies mentioned in the context that are expanding into GPU technologies?  \n",
       "6                                        What feature of AMD's Ryzen processors has contributed to their popularity among content creators?  \n",
       "7                             What was the impact of AMD's EPYC server processors on the data center market according to the given context?  \n",
       "8                                                                        What challenge has el faced in adopting newer nanometer processes?  \n",
       "9                                             How does the multi-core performance of AMD's Ryzen processors benefit content creation tasks?  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_with_document_chunk_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenTelemetry Tracing Details\n",
      "|  Phoenix Project: hugging-face\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n",
      "Running 27 questions‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG batch:   0%|          | 0/27 [00:00<?, ?q/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "RAG batch:   0%|          | 0/27 [00:12<?, ?q/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     _ = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     43\u001b[39m span.set_attribute(\u001b[33m\"\u001b[39m\u001b[33minput.value\u001b[39m\u001b[33m\"\u001b[39m, query)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     46\u001b[39m     \u001b[38;5;66;03m# t·∫°o prompt t·ª´ RAG (l·∫•y context top-k cho c√¢u h·ªèi)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     prompt = \u001b[43mbuild_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# g·ªçi model\u001b[39;00m\n\u001b[32m     50\u001b[39m     msg = ResponseInput(prompt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\tools\\rag.py:26\u001b[39m, in \u001b[36mbuild_prompt\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m     24\u001b[39m span.set_attribute(\u001b[33m\"\u001b[39m\u001b[33minput.value\u001b[39m\u001b[33m\"\u001b[39m, query)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     chunks = \u001b[43mretrieve_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     context = (\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks, start=\u001b[32m1\u001b[39m)])\n\u001b[32m     29\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m[1] (no context)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m     )\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# log ids + preview context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\tools\\retriever.py:61\u001b[39m, in \u001b[36mretrieve_chunks\u001b[39m\u001b[34m(query, top_k, fetch_k)\u001b[39m\n\u001b[32m     58\u001b[39m     span.add_event(notification)\n\u001b[32m     60\u001b[39m pairs = [[query, t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m scores = \u001b[43mreranker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m ranked = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mzip\u001b[39m(ids, docs, metas, scores), key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m3\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     64\u001b[39m top = ranked[:top_k]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\FlagEmbedding\\abc\\inference\\AbsReranker.py:218\u001b[39m, in \u001b[36mAbsReranker.compute_score\u001b[39m\u001b[34m(self, sentence_pairs, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m sentence_pairs = \u001b[38;5;28mself\u001b[39m.get_detailed_inputs(sentence_pairs)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentence_pairs, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.target_devices) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_score_single_gpu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43msentence_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_devices\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28mself\u001b[39m.pool = \u001b[38;5;28mself\u001b[39m.start_multi_process_pool()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\FlagEmbedding\\inference\\reranker\\encoder_only\\base.py:187\u001b[39m, in \u001b[36mBaseReranker.compute_score_single_gpu\u001b[39m\u001b[34m(self, sentence_pairs, batch_size, query_max_length, max_length, normalize, device, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m     sentences_batch = all_inputs_sorted[start_index:start_index + batch_size]\n\u001b[32m    180\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer.pad(\n\u001b[32m    181\u001b[39m         sentences_batch,\n\u001b[32m    182\u001b[39m         padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    183\u001b[39m         return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    184\u001b[39m         **kwargs\n\u001b[32m    185\u001b[39m     ).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.logits.view(-\u001b[32m1\u001b[39m, ).float()\n\u001b[32m    188\u001b[39m     all_scores.extend(scores.cpu().numpy().tolist())\n\u001b[32m    190\u001b[39m all_scores = [all_scores[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np.argsort(length_sorted_idx)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:1189\u001b[39m, in \u001b[36mXLMRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1172\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1174\u001b[39m \u001b[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1185\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1186\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1187\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1201\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:859\u001b[39m, in \u001b[36mXLMRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    853\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    854\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    857\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    872\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    873\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:610\u001b[39m, in \u001b[36mXLMRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    606\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    608\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:548\u001b[39m, in \u001b[36mXLMRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[39m\n\u001b[32m    545\u001b[39m     attention_output = cross_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    546\u001b[39m     outputs = outputs + cross_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:251\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:556\u001b[39m, in \u001b[36mXLMRobertaLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:471\u001b[39m, in \u001b[36mXLMRobertaIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Acer\\.vscode\\Technica_AI\\LiteLLM\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from opentelemetry.trace import Status, StatusCode\n",
    "# from Phoenix.trace.tracing import tracer\n",
    "\n",
    "# from LiteLLM.lite import LiteLLMClient\n",
    "# from LiteLLM.Response import ResponseInput\n",
    "# from tools.rag import build_prompt\n",
    "\n",
    "# # --------- CONFIG ---------\n",
    "# # df ngu·ªìn: l·∫•y c·ªôt \"question\"\n",
    "# SOURCE_DF = questions_with_document_chunk_df   # <- ƒë√£ c√≥ s·∫µn ·ªü ph√≠a b·∫°n\n",
    "# QUESTION_COL = \"question\"\n",
    "# TOP_K = 3          # build_prompt(query, top_k=TOP_K)\n",
    "# MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")  # ƒë·ªïi n·∫øu c·∫ßn\n",
    "# TEMP = float(os.getenv(\"LLM_TEMP\", \"0.2\"))\n",
    "# MAX_NUM = None     # gi·ªõi h·∫°n s·ªë c√¢u ch·∫°y (None = ch·∫°y h·∫øt)\n",
    "# # --------------------------\n",
    "\n",
    "# def main():\n",
    "#     # Chu·∫©n b·ªã danh s√°ch c√¢u h·ªèi\n",
    "#     queries = (\n",
    "#         SOURCE_DF[QUESTION_COL]\n",
    "#         .dropna()\n",
    "#         .map(lambda s: str(s).strip())\n",
    "#         .loc[lambda s: s.ne(\"\")]\n",
    "#         .tolist()\n",
    "#     )\n",
    "#     if MAX_NUM is not None:\n",
    "#         queries = queries[:MAX_NUM]\n",
    "\n",
    "#     print(f\"Running {len(queries)} questions‚Ä¶\")\n",
    "\n",
    "#     # Client LLM (tu·ª≥ m√¥i tr∆∞·ªùng b·∫°n c·∫•u h√¨nh API key/endpoint)\n",
    "#     client = LiteLLMClient()\n",
    "\n",
    "#     rows = []  # l∆∞u k·∫øt qu·∫£\n",
    "\n",
    "#     for i, query in enumerate(tqdm(queries, desc=\"RAG batch\", unit=\"q\")):\n",
    "#         with tracer.start_as_current_span(\"Thought\") as span:\n",
    "#             span.set_attribute(\"openinference.span.kind\", \"CHAIN\")\n",
    "#             span.set_attribute(\"input.value\", query)\n",
    "\n",
    "#             try:\n",
    "#                 # t·∫°o prompt t·ª´ RAG (l·∫•y context top-k cho c√¢u h·ªèi)\n",
    "#                 prompt = build_prompt(query, top_k=TOP_K)\n",
    "\n",
    "#                 # g·ªçi model\n",
    "#                 msg = ResponseInput(prompt)\n",
    "#                 resp = client.complete([msg])           # one-by-one (·ªïn ƒë·ªãnh)\n",
    "#                 answer = resp.transform()\n",
    "#                 usage = None\n",
    "#                 try:\n",
    "#                     usage = resp.usage()\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "\n",
    "#                 # log l√™n trace\n",
    "#                 span.set_attribute(\"output.value\", (answer or \"\")[:400])\n",
    "#                 span.set_status(Status(StatusCode.OK))\n",
    "\n",
    "#                 rows.append({\n",
    "#                     \"index\": i,\n",
    "#                     \"question\": query,\n",
    "#                     \"prompt\": prompt,\n",
    "#                     \"answer\": answer,\n",
    "#                     \"usage\": usage,\n",
    "#                     \"error\": None,\n",
    "#                 })\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 # ghi l·ªói nh∆∞ng kh√¥ng d·ª´ng batch\n",
    "#                 span.record_exception(e)\n",
    "#                 span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "#                 rows.append({\n",
    "#                     \"index\": i,\n",
    "#                     \"question\": query,\n",
    "#                     \"prompt\": None,\n",
    "#                     \"answer\": None,\n",
    "#                     \"usage\": None,\n",
    "#                     \"error\": str(e),\n",
    "#                 })\n",
    "\n",
    "#     # Th√†nh DataFrame k·∫øt qu·∫£\n",
    "#     results_df = pd.DataFrame(rows)\n",
    "#     pd.set_option(\"display.max_colwidth\", None)\n",
    "#     print(\"\\n=== SAMPLE RESULTS ===\")\n",
    "#     print(results_df.head(10)[[\"question\", \"answer\", \"error\"]])\n",
    "\n",
    "#     # (tu·ª≥ ch·ªçn) l∆∞u file\n",
    "#     # results_df.to_csv(\"rag_batch_results.csv\", index=False)\n",
    "#     return results_df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     _ = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9062b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['context.span_id', 'context.trace_id', 'input.value',\n",
      "       'retrieval.documents'],\n",
      "      dtype='object')\n",
      "    context.span_id                  context.trace_id  \\\n",
      "0  0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
      "1  6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
      "2  6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
      "3  4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
      "4  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
      "\n",
      "                                         input.value  \\\n",
      "0               I want to know information about AMD   \n",
      "1                I want some information about Intel   \n",
      "2  What feature of AMD's Ryzen processors has con...   \n",
      "3  What was the impact of AMD's EPYC server proce...   \n",
      "4  What are the two companies mentioned in the co...   \n",
      "\n",
      "                                 retrieval.documents  \n",
      "0  [{'document': {'content': '\n",
      "    \"AMD's Ryzen p...  \n",
      "1  [{'document': {'content': 'ntel‚Äôs in both pric...  \n",
      "2  [{'document': {'content': '\n",
      "    \"AMD's Ryzen p...  \n",
      "3  [{'document': {'content': 'nerations. \"\n",
      "    \"T...  \n",
      "4  [{'document': {'content': 'nd pre-built PCs. \"...  \n"
     ]
    }
   ],
   "source": [
    "client = px.Client()\n",
    "\n",
    "query = (\n",
    "    SpanQuery()\n",
    "    .where(\"span_kind == 'RETRIEVER' and evals['relevance'].label is None   \")\n",
    "    .select(\"context.span_id\", \"context.trace_id\", \"input.value\", \"retrieval.documents\")\n",
    ")\n",
    "\n",
    "df = client.query_spans(query, project_name=\"hugging-face\")\n",
    "df = df.reset_index().rename(columns={\"index\": \"context.span_id\"})\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a395158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['context.span_id', 'context.trace_id', 'input.value',\n",
      "       'retrieval.documents'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_exploded = df.explode(\"retrieval.documents\", ignore_index=True)\n",
    "\n",
    "print(df_exploded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee6ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['context.span_id', 'context.trace_id', 'input.value',\n",
      "       'document.content', 'document.id', 'document.score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# normalize nested dict\n",
    "docs = pd.json_normalize(df_exploded[\"retrieval.documents\"])\n",
    "\n",
    "# g·ªôp l·∫°i v·ªõi trace_id\n",
    "retrieved_documents_df = pd.concat([df_exploded.drop(columns=[\"retrieval.documents\"]), docs], axis=1)\n",
    "\n",
    "print(retrieved_documents_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a950b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents_df = retrieved_documents_df.rename(\n",
    "    columns={\n",
    "        \"document.content\": \"reference\",\n",
    "        \"input.value\": \"input\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f23c3bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document.id</th>\n",
       "      <th>document.score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.025658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.007090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.004895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.023789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.023743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>t, Intel still maintains strong relationships ...</td>\n",
       "      <td>dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9</td>\n",
       "      <td>0.010969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.995588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.018183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.000928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.977319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.006412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.001090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.992121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.005779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>n Radeon graphics and Intel launching its ARC ...</td>\n",
       "      <td>a3bb85cc-4987-415a-a1c6-e41f7c2a07d1</td>\n",
       "      <td>0.004970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.978433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.949401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.112510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.988813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.050221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>ffering higher core counts and better performa...</td>\n",
       "      <td>1329e0bc-f688-4192-af86-c2259c6168ca</td>\n",
       "      <td>0.007747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>t, Intel still maintains strong relationships ...</td>\n",
       "      <td>dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9</td>\n",
       "      <td>0.843321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.112517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.037068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.996719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.016507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.966074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.001705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>TSMC‚Äôs 7nm and 5nm technology to stay ahead i...</td>\n",
       "      <td>0511cc03-6907-47b7-a5b6-2e59816aa6b9</td>\n",
       "      <td>0.001625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.957924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>n Radeon graphics and Intel launching its ARC ...</td>\n",
       "      <td>a3bb85cc-4987-415a-a1c6-e41f7c2a07d1</td>\n",
       "      <td>0.398520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.006537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.649439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.034376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.027596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.092397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.065734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>TSMC‚Äôs 7nm and 5nm technology to stay ahead i...</td>\n",
       "      <td>0511cc03-6907-47b7-a5b6-2e59816aa6b9</td>\n",
       "      <td>0.038693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     context.span_id                  context.trace_id  \\\n",
       "0   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "1   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "2   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "3   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "4   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "5   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "6   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "7   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "8   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "9   4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "10  4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "11  4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "12  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "13  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "14  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "15  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "16  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "17  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "18  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "19  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "20  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "21  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "22  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "23  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "24  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "25  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "26  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "27  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "28  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "29  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "30  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "31  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "32  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "33  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "34  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "35  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "36  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "37  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "38  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "\n",
       "                                                input  \\\n",
       "0                I want to know information about AMD   \n",
       "1                I want to know information about AMD   \n",
       "2                I want to know information about AMD   \n",
       "3                 I want some information about Intel   \n",
       "4                 I want some information about Intel   \n",
       "5                 I want some information about Intel   \n",
       "6   What feature of AMD's Ryzen processors has con...   \n",
       "7   What feature of AMD's Ryzen processors has con...   \n",
       "8   What feature of AMD's Ryzen processors has con...   \n",
       "9   What was the impact of AMD's EPYC server proce...   \n",
       "10  What was the impact of AMD's EPYC server proce...   \n",
       "11  What was the impact of AMD's EPYC server proce...   \n",
       "12  What are the two companies mentioned in the co...   \n",
       "13  What are the two companies mentioned in the co...   \n",
       "14  What are the two companies mentioned in the co...   \n",
       "15  According to the context, what are both AMD an...   \n",
       "16  According to the context, what are both AMD an...   \n",
       "17  According to the context, what are both AMD an...   \n",
       "18  What type of users is Intel's recent focus on ...   \n",
       "19  What type of users is Intel's recent focus on ...   \n",
       "20  What type of users is Intel's recent focus on ...   \n",
       "21  According to the context, what strategy does I...   \n",
       "22  According to the context, what strategy does I...   \n",
       "23  According to the context, what strategy does I...   \n",
       "24  How does the multi-core performance of AMD's R...   \n",
       "25  How does the multi-core performance of AMD's R...   \n",
       "26  How does the multi-core performance of AMD's R...   \n",
       "27  Based on the information provided, what specif...   \n",
       "28  Based on the information provided, what specif...   \n",
       "29  Based on the information provided, what specif...   \n",
       "30  Which company is focusing on Radeon graphics a...   \n",
       "31  Which company is focusing on Radeon graphics a...   \n",
       "32  Which company is focusing on Radeon graphics a...   \n",
       "33  How does the context describe the current stat...   \n",
       "34  How does the context describe the current stat...   \n",
       "35  How does the context describe the current stat...   \n",
       "36                                    AMD information   \n",
       "37                                    AMD information   \n",
       "38                                    AMD information   \n",
       "\n",
       "                                            reference  \\\n",
       "0   \\n    \"AMD's Ryzen processors have been praise...   \n",
       "1   nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "2   nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "3   ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "4   hem attractive for content creators and gamers...   \n",
       "5   t, Intel still maintains strong relationships ...   \n",
       "6   \\n    \"AMD's Ryzen processors have been praise...   \n",
       "7   hem attractive for content creators and gamers...   \n",
       "8    its efficiency cores and hybrid architecture,...   \n",
       "9   nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "10  s without owning its own fabs. \"\\n    \"While A...   \n",
       "11  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "12  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "13  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "14  n Radeon graphics and Intel launching its ARC ...   \n",
       "15  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "16  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "17  s without owning its own fabs. \"\\n    \"While A...   \n",
       "18  hem attractive for content creators and gamers...   \n",
       "19   its efficiency cores and hybrid architecture,...   \n",
       "20  ffering higher core counts and better performa...   \n",
       "21  t, Intel still maintains strong relationships ...   \n",
       "22  s without owning its own fabs. \"\\n    \"While A...   \n",
       "23  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "24  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "25  hem attractive for content creators and gamers...   \n",
       "26  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "27  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "28   its efficiency cores and hybrid architecture,...   \n",
       "29   TSMC‚Äôs 7nm and 5nm technology to stay ahead i...   \n",
       "30  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "31  n Radeon graphics and Intel launching its ARC ...   \n",
       "32  s without owning its own fabs. \"\\n    \"While A...   \n",
       "33  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "34  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "35  s without owning its own fabs. \"\\n    \"While A...   \n",
       "36  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "37  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "38   TSMC‚Äôs 7nm and 5nm technology to stay ahead i...   \n",
       "\n",
       "                             document.id  document.score  \n",
       "0   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.025658  \n",
       "1   825177e1-6c77-4de7-bb02-c332e8dbaf54        0.007090  \n",
       "2   a23b9fae-a936-451e-b8d5-573ab30f08dd        0.004895  \n",
       "3   def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.023789  \n",
       "4   36084d3b-9775-4e1b-87a6-c542507e0074        0.023743  \n",
       "5   dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9        0.010969  \n",
       "6   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.995588  \n",
       "7   36084d3b-9775-4e1b-87a6-c542507e0074        0.018183  \n",
       "8   a0149ea1-fb03-4f53-be3a-891702d05314        0.000928  \n",
       "9   825177e1-6c77-4de7-bb02-c332e8dbaf54        0.977319  \n",
       "10  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.006412  \n",
       "11  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.001090  \n",
       "12  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.992121  \n",
       "13  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.005779  \n",
       "14  a3bb85cc-4987-415a-a1c6-e41f7c2a07d1        0.004970  \n",
       "15  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.978433  \n",
       "16  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.949401  \n",
       "17  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.112510  \n",
       "18  36084d3b-9775-4e1b-87a6-c542507e0074        0.988813  \n",
       "19  a0149ea1-fb03-4f53-be3a-891702d05314        0.050221  \n",
       "20  1329e0bc-f688-4192-af86-c2259c6168ca        0.007747  \n",
       "21  dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9        0.843321  \n",
       "22  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.112517  \n",
       "23  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.037068  \n",
       "24  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.996719  \n",
       "25  36084d3b-9775-4e1b-87a6-c542507e0074        0.016507  \n",
       "26  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.000792  \n",
       "27  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.966074  \n",
       "28  a0149ea1-fb03-4f53-be3a-891702d05314        0.001705  \n",
       "29  0511cc03-6907-47b7-a5b6-2e59816aa6b9        0.001625  \n",
       "30  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.957924  \n",
       "31  a3bb85cc-4987-415a-a1c6-e41f7c2a07d1        0.398520  \n",
       "32  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.006537  \n",
       "33  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.649439  \n",
       "34  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.034376  \n",
       "35  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.027596  \n",
       "36  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.092397  \n",
       "37  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.065734  \n",
       "38  0511cc03-6907-47b7-a5b6-2e59816aa6b9        0.038693  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4e111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n",
      "run_evals |          | 0/39 (0.0%) | ‚è≥ 00:00<? | ?it/s\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:40 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:46 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8WqkzS-3NKUce-9747f4990f8828b8\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756092046,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 11521173766409462000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's Ryzen processors and their performance characteristics. This information is directly related to AMD, as Ryzen processors are a product line from AMD. Therefore, the reference text contains relevant information to answer the question.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 288,\n",
      "    \"completion_tokens\": 60,\n",
      "    \"total_tokens\": 348,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m10:20:46 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:46 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1602 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m10:20:46 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:46 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:46 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñé         | 1/39 (2.6%) | ‚è≥ 00:06<04:21 |  6.89s/it\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:47 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "\u001b[92m10:20:47 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nerations. \"\\n    \"The launch of AMD\\'s EPYC server processors disrupted the data center market, offering higher core counts and b\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1631 - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m10:20:47 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:47 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8WqmRV-3NKUce-9747f4a1acc7fa2f\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756092047,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 14229453881700205000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about AMD. The reference text mentions AMD's EPYC server processors and their impact on the data center market. This information is directly related to AMD's products and market presence, which can help answer the question about AMD.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 292,\n",
      "    \"completion_tokens\": 57,\n",
      "    \"total_tokens\": 349,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1602 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñå         | 2/39 (5.1%) | ‚è≥ 00:08<02:13 |  3.60s/it\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:48 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:48 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want to know information about AMD\\n    ************\\n    [Reference text]: nd pre-built PCs. \"\\n    \"Both companies are expanding into GPU technologies, with AMD focusing on Radeon graphics and Intel laun\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "\u001b[92m10:20:49 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1631 - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m10:20:49 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8Wqmnw-3NKUce-9747f4a9cfc35008\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756092048,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 12553769715726590000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about AMD. The reference text mentions AMD specifically, noting its focus on Radeon graphics. This information is directly relevant to answering the question about AMD.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 291,\n",
      "    \"completion_tokens\": 42,\n",
      "    \"total_tokens\": 333,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m10:20:49 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1602 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñä         | 3/39 (7.7%) | ‚è≥ 00:09<01:30 |  2.51s/it\u001b[92m10:20:49 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:49 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:49 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1631 - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m10:20:50 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:50 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: ntel‚Äôs in both price and raw performance. \"\\n    \"Looking forward, both AMD and Intel are betting on AI, high-performance computi\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:50 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8WqnBo-3NKUce-9747f4b23f1d599d\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756092050,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 4088487490168833000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about Intel. The reference text mentions Intel in the context of its pricing and performance, and also notes Intel's future focus on AI and high-performance computing. While the text does not provide extensive details, it does mention Intel and its areas of focus, which can be relevant to answering the question.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 293,\n",
      "    \"completion_tokens\": 72,\n",
      "    \"total_tokens\": 365,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1602 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà         | 4/39 (10.3%) | ‚è≥ 00:10<01:14 |  2.12s/it\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: hem attractive for content creators and gamers alike. \"\\n    \"Intel has been working on improving its efficiency cores and hybrid\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:51 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: hem attractive for content creators and gamers alike. \"\\n    \"Intel has been working on improving its efficiency cores and hybrid\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:51 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: hem attractive for content creators and gamers alike. \"\\n    \"Intel has been working on improving its efficiency cores and hybrid\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1631 - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m10:20:51 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:51 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8Wqnc2-3NKUce-9747f4bb0d92f4b5\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756092051,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 7327595322680596000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about Intel. The reference text mentions Intel and discusses its work on improving its efficiency cores and hybrid features, which are relevant to Intel's technological advancements. This information could help answer the question by providing details about Intel's current focus and capabilities.\\nLABEL: relevant\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 287,\n",
      "    \"completion_tokens\": 61,\n",
      "    \"total_tokens\": 348,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1602 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñé        | 5/39 (12.8%) | ‚è≥ 00:12<01:03 |  1.86s/it\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: t, Intel still maintains strong relationships with OEMs, ensuring a steady presence in laptops and pre-built PCs. \"\\n    \"Both co\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:52 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: t, Intel still maintains strong relationships with OEMs, ensuring a steady presence in laptops and pre-built PCs. \"\\n    \"Both co\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:52 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: I want some information about Intel\\n    ************\\n    [Reference text]: t, Intel still maintains strong relationships with OEMs, ensuring a steady presence in laptops and pre-built PCs. \"\\n    \"Both co\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1631 - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m10:20:52 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:52 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:53 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8Wqo1z-3NKUce-9747f4c3bc6a28b8\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756092052,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 13328222822450788000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks for information about Intel. The reference text mentions Intel's relationships with OEMs and its presence in laptops and pre-built PCs. While the information is specific to Intel, it does not directly provide broad information about the company such as its history, products, or financials. However, it does offer context about Intel's market position, which could be relevant to understanding the company's current standing. Given the limited scope of the question and the provided text, the information is tangentially relevant but not directly comprehensive.\\n\\nLABEL: \\\"relevant\\\"\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 291,\n",
      "    \"completion_tokens\": 114,\n",
      "    \"total_tokens\": 405,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m10:20:54 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1602 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñå        | 6/39 (15.4%) | ‚è≥ 00:13<00:58 |  1.78s/it\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:54 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "\u001b[92m10:20:54 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]: \\n    \"AMD\\'s Ryzen processors have been praised for their strong multi-core performance, making them attractive for content creat\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1631 - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m10:20:54 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:54 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8WqoaS-3NKUce-9747f4cdcbd707bc\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756092054,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 1797178949919902200,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks about a specific feature of AMD's Ryzen processors that has contributed to their popularity among content creators. The reference text mentions that these processors have strong multi-core performance, which is a feature that could contribute to their popularity. While the reference text does not explicitly state that this feature contributes to popularity among content creators, it provides a relevant feature that could be the answer. Therefore, the reference text contains information that can help answer the question.\\n\\nLABEL: \\\"relevant\\\"\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 296,\n",
      "    \"completion_tokens\": 99,\n",
      "    \"total_tokens\": 395,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1602 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñä        | 7/39 (17.9%) | ‚è≥ 00:15<00:53 |  1.68s/it\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]: hem attractive for content creators and gamers alike. \"\\n    \"Intel has been working on improving its efficiency cores and hybrid\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:55 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]: hem attractive for content creators and gamers alike. \"\\n    \"Intel has been working on improving its efficiency cores and hybrid\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:55 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:55 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:56 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]: hem attractive for content creators and gamers alike. \"\\n    \"Intel has been working on improving its efficiency cores and hybrid\\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:56 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:56 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1631 - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m10:20:56 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:56 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:56 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - RAW RESPONSE:\n",
      "{\n",
      "  \"id\": \"o8WqoxB-3NKUce-9747f4d7085bd677\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1756092056,\n",
      "  \"model\": \"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
      "  \"prompt\": [],\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"seed\": 17561482173125212000,\n",
      "      \"logprobs\": null,\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"EXPLANATION: The question asks about a specific feature of AMD's Ryzen processors that has contributed to their popularity among content creators. The reference text does not mention AMD, Ryzen processors, or any specific features that might contribute to their popularity. It only makes a general statement about processors being attractive for content creators and gamers, without providing any details. Therefore, the reference text does not contain information that can help answer the question.\\n\\nLABEL: \\\"unrelated\\\"\",\n",
      "        \"tool_calls\": []\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 296,\n",
      "    \"completion_tokens\": 92,\n",
      "    \"total_tokens\": 388,\n",
      "    \"cached_tokens\": 0\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: utils.py:1262 - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1602 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:669 - selected model name for cost calculation: huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:16<00:50 |  1.64s/it\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:388 - Returned custom cost for model=huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo - prompt_tokens_cost_usd_dollar: 0, completion_tokens_cost_usd_dollar: 0\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1231 - response_cost: 0.0\n",
      "\u001b[92m10:20:57 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1631 - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m10:20:57 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'stripped_model_name': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'combined_stripped_model_name': 'huggingface/Qwen/Qwen2.5-7B-Instruct-Turbo', 'custom_llm_provider': 'huggingface'}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:20:57 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4987 - model_info: {'key': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'max_tokens': None, 'max_input_tokens': None, 'max_output_tokens': None, 'input_cost_per_token': 0, 'output_cost_per_token': 0, 'litellm_provider': 'huggingface', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': None, 'supports_function_calling': None, 'supports_tool_choice': None, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_computer_use': None, 'supports_pdf_input': None}\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:17<00:50 |  1.64s/it \u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:57 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:57 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 1: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:20:58 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:18<00:50 |  1.64s/it \u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 2: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:20:58 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:18<00:50 |  1.64s/it \u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:58 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:58 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 3: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:20:59 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:19<00:50 |  1.64s/it \u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:20:59 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:20:59 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 4: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:21:00 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:20<00:50 |  1.64s/it \u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:21:00 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 5: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:21:00 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:21:01 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:20<00:50 |  1.64s/it \u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 6: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:21:01 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:21<00:50 |  1.64s/it \u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:21:01 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 7: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:21:01 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:21:02 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:21<00:50 |  1.64s/it \u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:21:02 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 8: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:21:02 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:22<00:50 |  1.64s/it \u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:21:02 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 9: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:21:02 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:21:03 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:22<00:50 |  1.64s/it \u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \u001b[92mlitellm.completion(model='huggingface/together/Qwen/Qwen2.5-7B-Instruct', messages=[{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], temperature=0.0, max_tokens=1024, top_p=1, num_retries=0, request_timeout=60)\u001b[0m\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - \n",
      "\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {}\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m10:21:03 - LiteLLM:INFO\u001b[0m: utils.py:3309 - \n",
      "LiteLLM completion() model= together/Qwen/Qwen2.5-7B-Instruct; provider = huggingface\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:3312 - \n",
      "LiteLLM: Params passed to completion() {'model': 'together/Qwen/Qwen2.5-7B-Instruct', 'functions': None, 'function_call': None, 'temperature': 0.0, 'top_p': 1, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': 1024, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'huggingface', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': 0, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:3315 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:349 - Final returned optional params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:474 - self.optional_params: {'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024, 'max_retries': 0}\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: utils.py:4682 - checking potential_model_names in litellm.model_cost: {'split_model': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'stripped_model_name': 'together/Qwen/Qwen2.5-7B-Instruct', 'combined_stripped_model_name': 'huggingface/together/Qwen/Qwen2.5-7B-Instruct', 'custom_llm_provider': 'huggingface'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Exception in worker on attempt 10: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Retrying...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_retries` is not supported. It will be ignored.\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:929 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://router.huggingface.co/together/v1/chat/completions \\\n",
      "-H 'content-type: application/json' -H 'Authorization: Be****MM' \\\n",
      "-d '{'model': 'Qwen/Qwen2.5-7B-Instruct-Turbo', 'messages': [{'content': '\\nYou are comparing a reference text to a question and trying to determine if the reference text\\ncontains information relevant to answering the question. Here is the data:\\n    [BEGIN DATA]\\n    ************\\n    [Question]: What feature of AMD\\'s Ryzen processors has contributed to their popularity among content creators?\\n    ************\\n    [Reference text]:  its efficiency cores and hybrid architecture, especially with the Alder Lake and Raptor Lake generations. \"\\n    \"The launch of \\n    ************\\n    [END DATA]\\nCompare the Question above to the Reference text. You must determine whether the Reference text\\ncontains information that can help answer the Question. First, write out in a step by step manner\\nan EXPLANATION to show how to arrive at the correct answer. Avoid simply stating the correct answer\\nat the outset. Your response LABEL must be single word, either \"relevant\" or \"unrelated\", and\\nshould not contain any text or characters aside from that word. \"unrelated\" means that the\\nreference text does not help answer to the Question. \"relevant\" means the reference text directly\\nanswers the question.\\n\\nExample response:\\n************\\nEXPLANATION: An explanation of your reasoning for why the label is \"relevant\" or \"unrelated\"\\nLABEL: \"relevant\" or \"unrelated\"\\n************\\n\\nEXPLANATION:', 'role': 'user'}], 'temperature': 0.0, 'top_p': 1, 'max_tokens': 1024}'\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: get_api_base.py:62 - Error occurred in getting api base - litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=together/Qwen/Qwen2.5-7B-Instruct\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2330 - Logging Details: logger_fn - None | callable(logger_fn) - False\n",
      "\u001b[92m10:21:03 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2456 - Logging Details LiteLLM-Failure Call: []\n",
      "\u001b[92m10:21:03 - LiteLLM Proxy:DEBUG\u001b[0m: cold_storage_handler.py:78 - Unable to import proxy_server for cold storage logging: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`\n",
      "run_evals |‚ñà‚ñà        | 8/39 (20.5%) | ‚è≥ 00:23<00:50 |  1.64s/it "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "Retries exhausted after 11 attempts: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals import (\n",
    "    RelevanceEvaluator,\n",
    "    run_evals,\n",
    ")\n",
    "\n",
    "model = LiteLLMModel(\n",
    "    model=\"huggingface/together/Qwen/Qwen2.5-7B-Instruct\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "relevance_evaluator = RelevanceEvaluator(model)\n",
    "\n",
    "retrieved_documents_relevance_df = run_evals(\n",
    "    evaluators=[relevance_evaluator],\n",
    "    dataframe=retrieved_documents_df,\n",
    "    provide_explanation=True,\n",
    "    concurrency=20,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc7dedb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  score                                        explanation\n",
       "0  relevant    1.0  EXPLANATION: The question asks for information...\n",
       "1  relevant    1.0  EXPLANATION: The question asks for information...\n",
       "2  relevant    1.0  EXPLANATION: The question asks for information...\n",
       "3  relevant    1.0  EXPLANATION: The question asks for information...\n",
       "4  relevant    1.0  EXPLANATION: The question asks for information..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_relevance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980043dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context.span_id', 'context.trace_id', 'input', 'reference',\n",
       "       'document.id', 'document.score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e31bb370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document.id</th>\n",
       "      <th>document.score</th>\n",
       "      <th>eval_label</th>\n",
       "      <th>eval_score</th>\n",
       "      <th>eval_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.025658</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>t, Intel still maintains strong relationships ...</td>\n",
       "      <td>dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.995588</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks about a specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.018183</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EXPLANATION: The question asks about a specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.977319</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.992121</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>n Radeon graphics and Intel launching its ARC ...</td>\n",
       "      <td>a3bb85cc-4987-415a-a1c6-e41f7c2a07d1</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.978433</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.949401</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.112510</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.988813</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.050221</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>ffering higher core counts and better performa...</td>\n",
       "      <td>1329e0bc-f688-4192-af86-c2259c6168ca</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>t, Intel still maintains strong relationships ...</td>\n",
       "      <td>dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9</td>\n",
       "      <td>0.843321</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.112517</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.037068</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.996719</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.016507</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.966074</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>TSMC‚Äôs 7nm and 5nm technology to stay ahead i...</td>\n",
       "      <td>0511cc03-6907-47b7-a5b6-2e59816aa6b9</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.957924</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>n Radeon graphics and Intel launching its ARC ...</td>\n",
       "      <td>a3bb85cc-4987-415a-a1c6-e41f7c2a07d1</td>\n",
       "      <td>0.398520</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.649439</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.034376</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.092397</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.065734</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>TSMC‚Äôs 7nm and 5nm technology to stay ahead i...</td>\n",
       "      <td>0511cc03-6907-47b7-a5b6-2e59816aa6b9</td>\n",
       "      <td>0.038693</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     context.span_id                  context.trace_id  \\\n",
       "0   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "1   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "2   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "3   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "4   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "5   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "6   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "7   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "8   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "9   4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "10  4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "11  4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "12  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "13  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "14  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "15  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "16  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "17  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "18  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "19  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "20  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "21  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "22  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "23  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "24  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "25  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "26  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "27  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "28  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "29  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "30  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "31  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "32  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "33  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "34  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "35  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "36  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "37  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "38  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "\n",
       "                                                input  \\\n",
       "0                I want to know information about AMD   \n",
       "1                I want to know information about AMD   \n",
       "2                I want to know information about AMD   \n",
       "3                 I want some information about Intel   \n",
       "4                 I want some information about Intel   \n",
       "5                 I want some information about Intel   \n",
       "6   What feature of AMD's Ryzen processors has con...   \n",
       "7   What feature of AMD's Ryzen processors has con...   \n",
       "8   What feature of AMD's Ryzen processors has con...   \n",
       "9   What was the impact of AMD's EPYC server proce...   \n",
       "10  What was the impact of AMD's EPYC server proce...   \n",
       "11  What was the impact of AMD's EPYC server proce...   \n",
       "12  What are the two companies mentioned in the co...   \n",
       "13  What are the two companies mentioned in the co...   \n",
       "14  What are the two companies mentioned in the co...   \n",
       "15  According to the context, what are both AMD an...   \n",
       "16  According to the context, what are both AMD an...   \n",
       "17  According to the context, what are both AMD an...   \n",
       "18  What type of users is Intel's recent focus on ...   \n",
       "19  What type of users is Intel's recent focus on ...   \n",
       "20  What type of users is Intel's recent focus on ...   \n",
       "21  According to the context, what strategy does I...   \n",
       "22  According to the context, what strategy does I...   \n",
       "23  According to the context, what strategy does I...   \n",
       "24  How does the multi-core performance of AMD's R...   \n",
       "25  How does the multi-core performance of AMD's R...   \n",
       "26  How does the multi-core performance of AMD's R...   \n",
       "27  Based on the information provided, what specif...   \n",
       "28  Based on the information provided, what specif...   \n",
       "29  Based on the information provided, what specif...   \n",
       "30  Which company is focusing on Radeon graphics a...   \n",
       "31  Which company is focusing on Radeon graphics a...   \n",
       "32  Which company is focusing on Radeon graphics a...   \n",
       "33  How does the context describe the current stat...   \n",
       "34  How does the context describe the current stat...   \n",
       "35  How does the context describe the current stat...   \n",
       "36                                    AMD information   \n",
       "37                                    AMD information   \n",
       "38                                    AMD information   \n",
       "\n",
       "                                            reference  \\\n",
       "0   \\n    \"AMD's Ryzen processors have been praise...   \n",
       "1   nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "2   nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "3   ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "4   hem attractive for content creators and gamers...   \n",
       "5   t, Intel still maintains strong relationships ...   \n",
       "6   \\n    \"AMD's Ryzen processors have been praise...   \n",
       "7   hem attractive for content creators and gamers...   \n",
       "8    its efficiency cores and hybrid architecture,...   \n",
       "9   nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "10  s without owning its own fabs. \"\\n    \"While A...   \n",
       "11  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "12  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "13  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "14  n Radeon graphics and Intel launching its ARC ...   \n",
       "15  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "16  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "17  s without owning its own fabs. \"\\n    \"While A...   \n",
       "18  hem attractive for content creators and gamers...   \n",
       "19   its efficiency cores and hybrid architecture,...   \n",
       "20  ffering higher core counts and better performa...   \n",
       "21  t, Intel still maintains strong relationships ...   \n",
       "22  s without owning its own fabs. \"\\n    \"While A...   \n",
       "23  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "24  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "25  hem attractive for content creators and gamers...   \n",
       "26  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "27  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "28   its efficiency cores and hybrid architecture,...   \n",
       "29   TSMC‚Äôs 7nm and 5nm technology to stay ahead i...   \n",
       "30  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "31  n Radeon graphics and Intel launching its ARC ...   \n",
       "32  s without owning its own fabs. \"\\n    \"While A...   \n",
       "33  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "34  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "35  s without owning its own fabs. \"\\n    \"While A...   \n",
       "36  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "37  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "38   TSMC‚Äôs 7nm and 5nm technology to stay ahead i...   \n",
       "\n",
       "                             document.id  document.score eval_label  \\\n",
       "0   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.025658   relevant   \n",
       "1   825177e1-6c77-4de7-bb02-c332e8dbaf54        0.007090   relevant   \n",
       "2   a23b9fae-a936-451e-b8d5-573ab30f08dd        0.004895   relevant   \n",
       "3   def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.023789   relevant   \n",
       "4   36084d3b-9775-4e1b-87a6-c542507e0074        0.023743   relevant   \n",
       "5   dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9        0.010969   relevant   \n",
       "6   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.995588   relevant   \n",
       "7   36084d3b-9775-4e1b-87a6-c542507e0074        0.018183  unrelated   \n",
       "8   a0149ea1-fb03-4f53-be3a-891702d05314        0.000928       None   \n",
       "9   825177e1-6c77-4de7-bb02-c332e8dbaf54        0.977319       None   \n",
       "10  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.006412       None   \n",
       "11  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.001090       None   \n",
       "12  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.992121       None   \n",
       "13  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.005779       None   \n",
       "14  a3bb85cc-4987-415a-a1c6-e41f7c2a07d1        0.004970       None   \n",
       "15  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.978433       None   \n",
       "16  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.949401       None   \n",
       "17  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.112510       None   \n",
       "18  36084d3b-9775-4e1b-87a6-c542507e0074        0.988813       None   \n",
       "19  a0149ea1-fb03-4f53-be3a-891702d05314        0.050221       None   \n",
       "20  1329e0bc-f688-4192-af86-c2259c6168ca        0.007747       None   \n",
       "21  dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9        0.843321       None   \n",
       "22  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.112517       None   \n",
       "23  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.037068       None   \n",
       "24  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.996719       None   \n",
       "25  36084d3b-9775-4e1b-87a6-c542507e0074        0.016507       None   \n",
       "26  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.000792       None   \n",
       "27  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.966074       None   \n",
       "28  a0149ea1-fb03-4f53-be3a-891702d05314        0.001705       None   \n",
       "29  0511cc03-6907-47b7-a5b6-2e59816aa6b9        0.001625       None   \n",
       "30  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.957924       None   \n",
       "31  a3bb85cc-4987-415a-a1c6-e41f7c2a07d1        0.398520       None   \n",
       "32  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.006537       None   \n",
       "33  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.649439       None   \n",
       "34  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.034376       None   \n",
       "35  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.027596       None   \n",
       "36  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.092397       None   \n",
       "37  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.065734       None   \n",
       "38  0511cc03-6907-47b7-a5b6-2e59816aa6b9        0.038693       None   \n",
       "\n",
       "    eval_score                                   eval_explanation  \n",
       "0          1.0  EXPLANATION: The question asks for information...  \n",
       "1          1.0  EXPLANATION: The question asks for information...  \n",
       "2          1.0  EXPLANATION: The question asks for information...  \n",
       "3          1.0  EXPLANATION: The question asks for information...  \n",
       "4          1.0  EXPLANATION: The question asks for information...  \n",
       "5          1.0  EXPLANATION: The question asks for information...  \n",
       "6          1.0  EXPLANATION: The question asks about a specifi...  \n",
       "7          0.0  EXPLANATION: The question asks about a specifi...  \n",
       "8          NaN                                               None  \n",
       "9          NaN                                               None  \n",
       "10         NaN                                               None  \n",
       "11         NaN                                               None  \n",
       "12         NaN                                               None  \n",
       "13         NaN                                               None  \n",
       "14         NaN                                               None  \n",
       "15         NaN                                               None  \n",
       "16         NaN                                               None  \n",
       "17         NaN                                               None  \n",
       "18         NaN                                               None  \n",
       "19         NaN                                               None  \n",
       "20         NaN                                               None  \n",
       "21         NaN                                               None  \n",
       "22         NaN                                               None  \n",
       "23         NaN                                               None  \n",
       "24         NaN                                               None  \n",
       "25         NaN                                               None  \n",
       "26         NaN                                               None  \n",
       "27         NaN                                               None  \n",
       "28         NaN                                               None  \n",
       "29         NaN                                               None  \n",
       "30         NaN                                               None  \n",
       "31         NaN                                               None  \n",
       "32         NaN                                               None  \n",
       "33         NaN                                               None  \n",
       "34         NaN                                               None  \n",
       "35         NaN                                               None  \n",
       "36         NaN                                               None  \n",
       "37         NaN                                               None  \n",
       "38         NaN                                               None  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_with_relevance_df = pd.concat(\n",
    "    [retrieved_documents_df, retrieved_documents_relevance_df.add_prefix(\"eval_\")], axis=1\n",
    ")\n",
    "documents_with_relevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bfe8c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    context.span_id                  context.trace_id  \\\n",
      "0  0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
      "1  0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
      "2  0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
      "3  6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
      "4  6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
      "\n",
      "                                  input  \\\n",
      "0  I want to know information about AMD   \n",
      "1  I want to know information about AMD   \n",
      "2  I want to know information about AMD   \n",
      "3   I want some information about Intel   \n",
      "4   I want some information about Intel   \n",
      "\n",
      "                                           reference  \\\n",
      "0  \\n    \"AMD's Ryzen processors have been praise...   \n",
      "1  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
      "2  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
      "3  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
      "4  hem attractive for content creators and gamers...   \n",
      "\n",
      "                            document.id  document_score     label  score  \\\n",
      "0  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.025658  relevant    1.0   \n",
      "1  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.007090  relevant    1.0   \n",
      "2  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.004895  relevant    1.0   \n",
      "3  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.023789  relevant    1.0   \n",
      "4  36084d3b-9775-4e1b-87a6-c542507e0074        0.023743  relevant    1.0   \n",
      "\n",
      "                                         explanation  \n",
      "0  EXPLANATION: The question asks for information...  \n",
      "1  EXPLANATION: The question asks for information...  \n",
      "2  EXPLANATION: The question asks for information...  \n",
      "3  EXPLANATION: The question asks for information...  \n",
      "4  EXPLANATION: The question asks for information...  \n"
     ]
    }
   ],
   "source": [
    "documents_with_relevance_df = documents_with_relevance_df.rename(\n",
    "    columns={\n",
    "        \"eval_label\": \"label\",\n",
    "        \"eval_score\": \"score\",\n",
    "        \"eval_explanation\": \"explanation\",\n",
    "        \"document.score\": \"document_score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(documents_with_relevance_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c527b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document.id</th>\n",
       "      <th>document_score</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.025658</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0224631cead9f25d</td>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6fe4de290b8d7478</td>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>t, Intel still maintains strong relationships ...</td>\n",
       "      <td>dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.995588</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks about a specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.018183</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EXPLANATION: The question asks about a specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6b384c0a008fcdc1</td>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.977319</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4abdb3f2ef462568</td>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.992121</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5551e37d1a9fdeef</td>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>n Radeon graphics and Intel launching its ARC ...</td>\n",
       "      <td>a3bb85cc-4987-415a-a1c6-e41f7c2a07d1</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.978433</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.949401</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1ae2af8f3837139e</td>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.112510</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.988813</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.050221</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5115f7ecca3a55ea</td>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>ffering higher core counts and better performa...</td>\n",
       "      <td>1329e0bc-f688-4192-af86-c2259c6168ca</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>t, Intel still maintains strong relationships ...</td>\n",
       "      <td>dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9</td>\n",
       "      <td>0.843321</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.112517</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>c60037afa8f849c4</td>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.037068</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.996719</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.016507</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1e9fbe7fd91c299d</td>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.966074</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1a81efb791e850df</td>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>TSMC‚Äôs 7nm and 5nm technology to stay ahead i...</td>\n",
       "      <td>0511cc03-6907-47b7-a5b6-2e59816aa6b9</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.957924</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>n Radeon graphics and Intel launching its ARC ...</td>\n",
       "      <td>a3bb85cc-4987-415a-a1c6-e41f7c2a07d1</td>\n",
       "      <td>0.398520</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7d64a5556cec5907</td>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.649439</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.034376</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>d0a79204813cc2bb</td>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.092397</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.065734</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>f336ec694fdcbba5</td>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>TSMC‚Äôs 7nm and 5nm technology to stay ahead i...</td>\n",
       "      <td>0511cc03-6907-47b7-a5b6-2e59816aa6b9</td>\n",
       "      <td>0.038693</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     context.span_id                  context.trace_id  \\\n",
       "0   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "1   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "2   0224631cead9f25d  500b3bdc9e9c658bee05bdea991a793a   \n",
       "3   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "4   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "5   6fe4de290b8d7478  582261bea40ac7541550f4369e7342b1   \n",
       "6   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "7   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "8   6b384c0a008fcdc1  2bb550be5926f49d23cef2b2443893d0   \n",
       "9   4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "10  4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "11  4abdb3f2ef462568  3969182ba42aa8040638bd5352d2339c   \n",
       "12  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "13  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "14  5551e37d1a9fdeef  58481aa2e6a562825c0a9c419ac0582d   \n",
       "15  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "16  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "17  1ae2af8f3837139e  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "18  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "19  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "20  5115f7ecca3a55ea  ffb70b1db7b98974928f300dbdd52973   \n",
       "21  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "22  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "23  c60037afa8f849c4  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "24  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "25  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "26  1e9fbe7fd91c299d  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "27  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "28  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "29  1a81efb791e850df  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "30  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "31  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "32  7d64a5556cec5907  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "33  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "34  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "35  d0a79204813cc2bb  b03c665c1e689e93c306a328dd9282e7   \n",
       "36  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "37  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "38  f336ec694fdcbba5  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "\n",
       "                                                input  \\\n",
       "0                I want to know information about AMD   \n",
       "1                I want to know information about AMD   \n",
       "2                I want to know information about AMD   \n",
       "3                 I want some information about Intel   \n",
       "4                 I want some information about Intel   \n",
       "5                 I want some information about Intel   \n",
       "6   What feature of AMD's Ryzen processors has con...   \n",
       "7   What feature of AMD's Ryzen processors has con...   \n",
       "8   What feature of AMD's Ryzen processors has con...   \n",
       "9   What was the impact of AMD's EPYC server proce...   \n",
       "10  What was the impact of AMD's EPYC server proce...   \n",
       "11  What was the impact of AMD's EPYC server proce...   \n",
       "12  What are the two companies mentioned in the co...   \n",
       "13  What are the two companies mentioned in the co...   \n",
       "14  What are the two companies mentioned in the co...   \n",
       "15  According to the context, what are both AMD an...   \n",
       "16  According to the context, what are both AMD an...   \n",
       "17  According to the context, what are both AMD an...   \n",
       "18  What type of users is Intel's recent focus on ...   \n",
       "19  What type of users is Intel's recent focus on ...   \n",
       "20  What type of users is Intel's recent focus on ...   \n",
       "21  According to the context, what strategy does I...   \n",
       "22  According to the context, what strategy does I...   \n",
       "23  According to the context, what strategy does I...   \n",
       "24  How does the multi-core performance of AMD's R...   \n",
       "25  How does the multi-core performance of AMD's R...   \n",
       "26  How does the multi-core performance of AMD's R...   \n",
       "27  Based on the information provided, what specif...   \n",
       "28  Based on the information provided, what specif...   \n",
       "29  Based on the information provided, what specif...   \n",
       "30  Which company is focusing on Radeon graphics a...   \n",
       "31  Which company is focusing on Radeon graphics a...   \n",
       "32  Which company is focusing on Radeon graphics a...   \n",
       "33  How does the context describe the current stat...   \n",
       "34  How does the context describe the current stat...   \n",
       "35  How does the context describe the current stat...   \n",
       "36                                    AMD information   \n",
       "37                                    AMD information   \n",
       "38                                    AMD information   \n",
       "\n",
       "                                            reference  \\\n",
       "0   \\n    \"AMD's Ryzen processors have been praise...   \n",
       "1   nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "2   nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "3   ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "4   hem attractive for content creators and gamers...   \n",
       "5   t, Intel still maintains strong relationships ...   \n",
       "6   \\n    \"AMD's Ryzen processors have been praise...   \n",
       "7   hem attractive for content creators and gamers...   \n",
       "8    its efficiency cores and hybrid architecture,...   \n",
       "9   nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "10  s without owning its own fabs. \"\\n    \"While A...   \n",
       "11  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "12  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "13  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "14  n Radeon graphics and Intel launching its ARC ...   \n",
       "15  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "16  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "17  s without owning its own fabs. \"\\n    \"While A...   \n",
       "18  hem attractive for content creators and gamers...   \n",
       "19   its efficiency cores and hybrid architecture,...   \n",
       "20  ffering higher core counts and better performa...   \n",
       "21  t, Intel still maintains strong relationships ...   \n",
       "22  s without owning its own fabs. \"\\n    \"While A...   \n",
       "23  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "24  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "25  hem attractive for content creators and gamers...   \n",
       "26  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "27  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "28   its efficiency cores and hybrid architecture,...   \n",
       "29   TSMC‚Äôs 7nm and 5nm technology to stay ahead i...   \n",
       "30  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "31  n Radeon graphics and Intel launching its ARC ...   \n",
       "32  s without owning its own fabs. \"\\n    \"While A...   \n",
       "33  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "34  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "35  s without owning its own fabs. \"\\n    \"While A...   \n",
       "36  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "37  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "38   TSMC‚Äôs 7nm and 5nm technology to stay ahead i...   \n",
       "\n",
       "                             document.id  document_score      label  score  \\\n",
       "0   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.025658   relevant    1.0   \n",
       "1   825177e1-6c77-4de7-bb02-c332e8dbaf54        0.007090   relevant    1.0   \n",
       "2   a23b9fae-a936-451e-b8d5-573ab30f08dd        0.004895   relevant    1.0   \n",
       "3   def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.023789   relevant    1.0   \n",
       "4   36084d3b-9775-4e1b-87a6-c542507e0074        0.023743   relevant    1.0   \n",
       "5   dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9        0.010969   relevant    1.0   \n",
       "6   6e13a8d5-0deb-4982-8f6a-601164f66a73        0.995588   relevant    1.0   \n",
       "7   36084d3b-9775-4e1b-87a6-c542507e0074        0.018183  unrelated    0.0   \n",
       "8   a0149ea1-fb03-4f53-be3a-891702d05314        0.000928       None    NaN   \n",
       "9   825177e1-6c77-4de7-bb02-c332e8dbaf54        0.977319       None    NaN   \n",
       "10  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.006412       None    NaN   \n",
       "11  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.001090       None    NaN   \n",
       "12  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.992121       None    NaN   \n",
       "13  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.005779       None    NaN   \n",
       "14  a3bb85cc-4987-415a-a1c6-e41f7c2a07d1        0.004970       None    NaN   \n",
       "15  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.978433       None    NaN   \n",
       "16  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.949401       None    NaN   \n",
       "17  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.112510       None    NaN   \n",
       "18  36084d3b-9775-4e1b-87a6-c542507e0074        0.988813       None    NaN   \n",
       "19  a0149ea1-fb03-4f53-be3a-891702d05314        0.050221       None    NaN   \n",
       "20  1329e0bc-f688-4192-af86-c2259c6168ca        0.007747       None    NaN   \n",
       "21  dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9        0.843321       None    NaN   \n",
       "22  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.112517       None    NaN   \n",
       "23  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.037068       None    NaN   \n",
       "24  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.996719       None    NaN   \n",
       "25  36084d3b-9775-4e1b-87a6-c542507e0074        0.016507       None    NaN   \n",
       "26  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.000792       None    NaN   \n",
       "27  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.966074       None    NaN   \n",
       "28  a0149ea1-fb03-4f53-be3a-891702d05314        0.001705       None    NaN   \n",
       "29  0511cc03-6907-47b7-a5b6-2e59816aa6b9        0.001625       None    NaN   \n",
       "30  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.957924       None    NaN   \n",
       "31  a3bb85cc-4987-415a-a1c6-e41f7c2a07d1        0.398520       None    NaN   \n",
       "32  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.006537       None    NaN   \n",
       "33  def89f0c-d7d3-4c57-964a-4689cfda6ad3        0.649439       None    NaN   \n",
       "34  a23b9fae-a936-451e-b8d5-573ab30f08dd        0.034376       None    NaN   \n",
       "35  207f6d67-3dbf-4aea-88dc-c9df94f43b43        0.027596       None    NaN   \n",
       "36  6e13a8d5-0deb-4982-8f6a-601164f66a73        0.092397       None    NaN   \n",
       "37  825177e1-6c77-4de7-bb02-c332e8dbaf54        0.065734       None    NaN   \n",
       "38  0511cc03-6907-47b7-a5b6-2e59816aa6b9        0.038693       None    NaN   \n",
       "\n",
       "                                          explanation  \n",
       "0   EXPLANATION: The question asks for information...  \n",
       "1   EXPLANATION: The question asks for information...  \n",
       "2   EXPLANATION: The question asks for information...  \n",
       "3   EXPLANATION: The question asks for information...  \n",
       "4   EXPLANATION: The question asks for information...  \n",
       "5   EXPLANATION: The question asks for information...  \n",
       "6   EXPLANATION: The question asks about a specifi...  \n",
       "7   EXPLANATION: The question asks about a specifi...  \n",
       "8                                                None  \n",
       "9                                                None  \n",
       "10                                               None  \n",
       "11                                               None  \n",
       "12                                               None  \n",
       "13                                               None  \n",
       "14                                               None  \n",
       "15                                               None  \n",
       "16                                               None  \n",
       "17                                               None  \n",
       "18                                               None  \n",
       "19                                               None  \n",
       "20                                               None  \n",
       "21                                               None  \n",
       "22                                               None  \n",
       "23                                               None  \n",
       "24                                               None  \n",
       "25                                               None  \n",
       "26                                               None  \n",
       "27                                               None  \n",
       "28                                               None  \n",
       "29                                               None  \n",
       "30                                               None  \n",
       "31                                               None  \n",
       "32                                               None  \n",
       "33                                               None  \n",
       "34                                               None  \n",
       "35                                               None  \n",
       "36                                               None  \n",
       "37                                               None  \n",
       "38                                               None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_with_relevance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05568b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "final_df = documents_with_relevance_df.copy()\n",
    "\n",
    "# √©p score v·ªÅ s·ªë, bi·∫øn l·ªói th√†nh NaN\n",
    "final_df['__score__'] = pd.to_numeric(final_df['document_score'], errors='coerce')\n",
    "\n",
    "# sort theo span_id v√† score gi·∫£m d·∫ßn; NaN ƒë·∫©y xu·ªëng cu·ªëi nh√≥m\n",
    "final_df = final_df.sort_values(\n",
    "    ['context.span_id', '__score__'], ascending=[True, False]\n",
    ")\n",
    "\n",
    "# ƒë√°nh s·ªë 0,1,2,... trong t·ª´ng span\n",
    "final_df['document_position'] = final_df.groupby('context.span_id').cumcount() \n",
    "\n",
    "# MultiIndex ƒë√∫ng chu·∫©n Phoenix (n·∫øu b·∫°n c·∫ßn)\n",
    "final_df = final_df.set_index(['context.span_id', 'document_position']).drop(columns='__score__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31bddd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['context.trace_id', 'input', 'reference', 'document.id',\n",
       "       'document_score', 'label', 'score', 'explanation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d1a5f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document.id</th>\n",
       "      <th>document_score</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0224631cead9f25d</th>\n",
       "      <th>0</th>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.025658</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500b3bdc9e9c658bee05bdea991a793a</td>\n",
       "      <td>I want to know information about AMD</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1a81efb791e850df</th>\n",
       "      <th>0</th>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.966074</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54b2e9d0aa6bd4e32494f66888f35770</td>\n",
       "      <td>Based on the information provided, what specif...</td>\n",
       "      <td>TSMC‚Äôs 7nm and 5nm technology to stay ahead i...</td>\n",
       "      <td>0511cc03-6907-47b7-a5b6-2e59816aa6b9</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1ae2af8f3837139e</th>\n",
       "      <th>0</th>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.978433</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.949401</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ef83a9f7bb737cac4040ca084b1f5418</td>\n",
       "      <td>According to the context, what are both AMD an...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.112510</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1e9fbe7fd91c299d</th>\n",
       "      <th>0</th>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.996719</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.016507</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cb3b1808165551a70eaa1a0edc6fd1a5</td>\n",
       "      <td>How does the multi-core performance of AMD's R...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4abdb3f2ef462568</th>\n",
       "      <th>0</th>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.977319</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3969182ba42aa8040638bd5352d2339c</td>\n",
       "      <td>What was the impact of AMD's EPYC server proce...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5115f7ecca3a55ea</th>\n",
       "      <th>0</th>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.988813</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.050221</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffb70b1db7b98974928f300dbdd52973</td>\n",
       "      <td>What type of users is Intel's recent focus on ...</td>\n",
       "      <td>ffering higher core counts and better performa...</td>\n",
       "      <td>1329e0bc-f688-4192-af86-c2259c6168ca</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5551e37d1a9fdeef</th>\n",
       "      <th>0</th>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.992121</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58481aa2e6a562825c0a9c419ac0582d</td>\n",
       "      <td>What are the two companies mentioned in the co...</td>\n",
       "      <td>n Radeon graphics and Intel launching its ARC ...</td>\n",
       "      <td>a3bb85cc-4987-415a-a1c6-e41f7c2a07d1</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">6b384c0a008fcdc1</th>\n",
       "      <th>0</th>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.995588</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks about a specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.018183</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EXPLANATION: The question asks about a specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2bb550be5926f49d23cef2b2443893d0</td>\n",
       "      <td>What feature of AMD's Ryzen processors has con...</td>\n",
       "      <td>its efficiency cores and hybrid architecture,...</td>\n",
       "      <td>a0149ea1-fb03-4f53-be3a-891702d05314</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">6fe4de290b8d7478</th>\n",
       "      <th>0</th>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.023789</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>hem attractive for content creators and gamers...</td>\n",
       "      <td>36084d3b-9775-4e1b-87a6-c542507e0074</td>\n",
       "      <td>0.023743</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>582261bea40ac7541550f4369e7342b1</td>\n",
       "      <td>I want some information about Intel</td>\n",
       "      <td>t, Intel still maintains strong relationships ...</td>\n",
       "      <td>dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>relevant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EXPLANATION: The question asks for information...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">7d64a5556cec5907</th>\n",
       "      <th>0</th>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.957924</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>n Radeon graphics and Intel launching its ARC ...</td>\n",
       "      <td>a3bb85cc-4987-415a-a1c6-e41f7c2a07d1</td>\n",
       "      <td>0.398520</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f641296b371e8b39b3c0cc36fca3deb6</td>\n",
       "      <td>Which company is focusing on Radeon graphics a...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.006537</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">c60037afa8f849c4</th>\n",
       "      <th>0</th>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>t, Intel still maintains strong relationships ...</td>\n",
       "      <td>dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9</td>\n",
       "      <td>0.843321</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.112517</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0153b84bd7b3cbc38a0d1e5a9323640f</td>\n",
       "      <td>According to the context, what strategy does I...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.037068</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">d0a79204813cc2bb</th>\n",
       "      <th>0</th>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>ntel‚Äôs in both price and raw performance. \"\\n ...</td>\n",
       "      <td>def89f0c-d7d3-4c57-964a-4689cfda6ad3</td>\n",
       "      <td>0.649439</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>nd pre-built PCs. \"\\n    \"Both companies are e...</td>\n",
       "      <td>a23b9fae-a936-451e-b8d5-573ab30f08dd</td>\n",
       "      <td>0.034376</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b03c665c1e689e93c306a328dd9282e7</td>\n",
       "      <td>How does the context describe the current stat...</td>\n",
       "      <td>s without owning its own fabs. \"\\n    \"While A...</td>\n",
       "      <td>207f6d67-3dbf-4aea-88dc-c9df94f43b43</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">f336ec694fdcbba5</th>\n",
       "      <th>0</th>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>\\n    \"AMD's Ryzen processors have been praise...</td>\n",
       "      <td>6e13a8d5-0deb-4982-8f6a-601164f66a73</td>\n",
       "      <td>0.092397</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>nerations. \"\\n    \"The launch of AMD's EPYC se...</td>\n",
       "      <td>825177e1-6c77-4de7-bb02-c332e8dbaf54</td>\n",
       "      <td>0.065734</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a223f8abc47a3f8f431cfb4851c306f9</td>\n",
       "      <td>AMD information</td>\n",
       "      <td>TSMC‚Äôs 7nm and 5nm technology to stay ahead i...</td>\n",
       "      <td>0511cc03-6907-47b7-a5b6-2e59816aa6b9</td>\n",
       "      <td>0.038693</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "0224631cead9f25d 0                  500b3bdc9e9c658bee05bdea991a793a   \n",
       "                 1                  500b3bdc9e9c658bee05bdea991a793a   \n",
       "                 2                  500b3bdc9e9c658bee05bdea991a793a   \n",
       "1a81efb791e850df 0                  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "                 1                  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "                 2                  54b2e9d0aa6bd4e32494f66888f35770   \n",
       "1ae2af8f3837139e 0                  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "                 1                  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "                 2                  ef83a9f7bb737cac4040ca084b1f5418   \n",
       "1e9fbe7fd91c299d 0                  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "                 1                  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "                 2                  cb3b1808165551a70eaa1a0edc6fd1a5   \n",
       "4abdb3f2ef462568 0                  3969182ba42aa8040638bd5352d2339c   \n",
       "                 1                  3969182ba42aa8040638bd5352d2339c   \n",
       "                 2                  3969182ba42aa8040638bd5352d2339c   \n",
       "5115f7ecca3a55ea 0                  ffb70b1db7b98974928f300dbdd52973   \n",
       "                 1                  ffb70b1db7b98974928f300dbdd52973   \n",
       "                 2                  ffb70b1db7b98974928f300dbdd52973   \n",
       "5551e37d1a9fdeef 0                  58481aa2e6a562825c0a9c419ac0582d   \n",
       "                 1                  58481aa2e6a562825c0a9c419ac0582d   \n",
       "                 2                  58481aa2e6a562825c0a9c419ac0582d   \n",
       "6b384c0a008fcdc1 0                  2bb550be5926f49d23cef2b2443893d0   \n",
       "                 1                  2bb550be5926f49d23cef2b2443893d0   \n",
       "                 2                  2bb550be5926f49d23cef2b2443893d0   \n",
       "6fe4de290b8d7478 0                  582261bea40ac7541550f4369e7342b1   \n",
       "                 1                  582261bea40ac7541550f4369e7342b1   \n",
       "                 2                  582261bea40ac7541550f4369e7342b1   \n",
       "7d64a5556cec5907 0                  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "                 1                  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "                 2                  f641296b371e8b39b3c0cc36fca3deb6   \n",
       "c60037afa8f849c4 0                  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "                 1                  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "                 2                  0153b84bd7b3cbc38a0d1e5a9323640f   \n",
       "d0a79204813cc2bb 0                  b03c665c1e689e93c306a328dd9282e7   \n",
       "                 1                  b03c665c1e689e93c306a328dd9282e7   \n",
       "                 2                  b03c665c1e689e93c306a328dd9282e7   \n",
       "f336ec694fdcbba5 0                  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "                 1                  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "                 2                  a223f8abc47a3f8f431cfb4851c306f9   \n",
       "\n",
       "                                                                                input  \\\n",
       "context.span_id  document_position                                                      \n",
       "0224631cead9f25d 0                               I want to know information about AMD   \n",
       "                 1                               I want to know information about AMD   \n",
       "                 2                               I want to know information about AMD   \n",
       "1a81efb791e850df 0                  Based on the information provided, what specif...   \n",
       "                 1                  Based on the information provided, what specif...   \n",
       "                 2                  Based on the information provided, what specif...   \n",
       "1ae2af8f3837139e 0                  According to the context, what are both AMD an...   \n",
       "                 1                  According to the context, what are both AMD an...   \n",
       "                 2                  According to the context, what are both AMD an...   \n",
       "1e9fbe7fd91c299d 0                  How does the multi-core performance of AMD's R...   \n",
       "                 1                  How does the multi-core performance of AMD's R...   \n",
       "                 2                  How does the multi-core performance of AMD's R...   \n",
       "4abdb3f2ef462568 0                  What was the impact of AMD's EPYC server proce...   \n",
       "                 1                  What was the impact of AMD's EPYC server proce...   \n",
       "                 2                  What was the impact of AMD's EPYC server proce...   \n",
       "5115f7ecca3a55ea 0                  What type of users is Intel's recent focus on ...   \n",
       "                 1                  What type of users is Intel's recent focus on ...   \n",
       "                 2                  What type of users is Intel's recent focus on ...   \n",
       "5551e37d1a9fdeef 0                  What are the two companies mentioned in the co...   \n",
       "                 1                  What are the two companies mentioned in the co...   \n",
       "                 2                  What are the two companies mentioned in the co...   \n",
       "6b384c0a008fcdc1 0                  What feature of AMD's Ryzen processors has con...   \n",
       "                 1                  What feature of AMD's Ryzen processors has con...   \n",
       "                 2                  What feature of AMD's Ryzen processors has con...   \n",
       "6fe4de290b8d7478 0                                I want some information about Intel   \n",
       "                 1                                I want some information about Intel   \n",
       "                 2                                I want some information about Intel   \n",
       "7d64a5556cec5907 0                  Which company is focusing on Radeon graphics a...   \n",
       "                 1                  Which company is focusing on Radeon graphics a...   \n",
       "                 2                  Which company is focusing on Radeon graphics a...   \n",
       "c60037afa8f849c4 0                  According to the context, what strategy does I...   \n",
       "                 1                  According to the context, what strategy does I...   \n",
       "                 2                  According to the context, what strategy does I...   \n",
       "d0a79204813cc2bb 0                  How does the context describe the current stat...   \n",
       "                 1                  How does the context describe the current stat...   \n",
       "                 2                  How does the context describe the current stat...   \n",
       "f336ec694fdcbba5 0                                                    AMD information   \n",
       "                 1                                                    AMD information   \n",
       "                 2                                                    AMD information   \n",
       "\n",
       "                                                                            reference  \\\n",
       "context.span_id  document_position                                                      \n",
       "0224631cead9f25d 0                  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "                 1                  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "                 2                  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "1a81efb791e850df 0                  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "                 1                   its efficiency cores and hybrid architecture,...   \n",
       "                 2                   TSMC‚Äôs 7nm and 5nm technology to stay ahead i...   \n",
       "1ae2af8f3837139e 0                  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "                 1                  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "                 2                  s without owning its own fabs. \"\\n    \"While A...   \n",
       "1e9fbe7fd91c299d 0                  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "                 1                  hem attractive for content creators and gamers...   \n",
       "                 2                  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "4abdb3f2ef462568 0                  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "                 1                  s without owning its own fabs. \"\\n    \"While A...   \n",
       "                 2                  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "5115f7ecca3a55ea 0                  hem attractive for content creators and gamers...   \n",
       "                 1                   its efficiency cores and hybrid architecture,...   \n",
       "                 2                  ffering higher core counts and better performa...   \n",
       "5551e37d1a9fdeef 0                  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "                 1                  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "                 2                  n Radeon graphics and Intel launching its ARC ...   \n",
       "6b384c0a008fcdc1 0                  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "                 1                  hem attractive for content creators and gamers...   \n",
       "                 2                   its efficiency cores and hybrid architecture,...   \n",
       "6fe4de290b8d7478 0                  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "                 1                  hem attractive for content creators and gamers...   \n",
       "                 2                  t, Intel still maintains strong relationships ...   \n",
       "7d64a5556cec5907 0                  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "                 1                  n Radeon graphics and Intel launching its ARC ...   \n",
       "                 2                  s without owning its own fabs. \"\\n    \"While A...   \n",
       "c60037afa8f849c4 0                  t, Intel still maintains strong relationships ...   \n",
       "                 1                  s without owning its own fabs. \"\\n    \"While A...   \n",
       "                 2                  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "d0a79204813cc2bb 0                  ntel‚Äôs in both price and raw performance. \"\\n ...   \n",
       "                 1                  nd pre-built PCs. \"\\n    \"Both companies are e...   \n",
       "                 2                  s without owning its own fabs. \"\\n    \"While A...   \n",
       "f336ec694fdcbba5 0                  \\n    \"AMD's Ryzen processors have been praise...   \n",
       "                 1                  nerations. \"\\n    \"The launch of AMD's EPYC se...   \n",
       "                 2                   TSMC‚Äôs 7nm and 5nm technology to stay ahead i...   \n",
       "\n",
       "                                                             document.id  \\\n",
       "context.span_id  document_position                                         \n",
       "0224631cead9f25d 0                  6e13a8d5-0deb-4982-8f6a-601164f66a73   \n",
       "                 1                  825177e1-6c77-4de7-bb02-c332e8dbaf54   \n",
       "                 2                  a23b9fae-a936-451e-b8d5-573ab30f08dd   \n",
       "1a81efb791e850df 0                  825177e1-6c77-4de7-bb02-c332e8dbaf54   \n",
       "                 1                  a0149ea1-fb03-4f53-be3a-891702d05314   \n",
       "                 2                  0511cc03-6907-47b7-a5b6-2e59816aa6b9   \n",
       "1ae2af8f3837139e 0                  def89f0c-d7d3-4c57-964a-4689cfda6ad3   \n",
       "                 1                  a23b9fae-a936-451e-b8d5-573ab30f08dd   \n",
       "                 2                  207f6d67-3dbf-4aea-88dc-c9df94f43b43   \n",
       "1e9fbe7fd91c299d 0                  6e13a8d5-0deb-4982-8f6a-601164f66a73   \n",
       "                 1                  36084d3b-9775-4e1b-87a6-c542507e0074   \n",
       "                 2                  825177e1-6c77-4de7-bb02-c332e8dbaf54   \n",
       "4abdb3f2ef462568 0                  825177e1-6c77-4de7-bb02-c332e8dbaf54   \n",
       "                 1                  207f6d67-3dbf-4aea-88dc-c9df94f43b43   \n",
       "                 2                  6e13a8d5-0deb-4982-8f6a-601164f66a73   \n",
       "5115f7ecca3a55ea 0                  36084d3b-9775-4e1b-87a6-c542507e0074   \n",
       "                 1                  a0149ea1-fb03-4f53-be3a-891702d05314   \n",
       "                 2                  1329e0bc-f688-4192-af86-c2259c6168ca   \n",
       "5551e37d1a9fdeef 0                  a23b9fae-a936-451e-b8d5-573ab30f08dd   \n",
       "                 1                  def89f0c-d7d3-4c57-964a-4689cfda6ad3   \n",
       "                 2                  a3bb85cc-4987-415a-a1c6-e41f7c2a07d1   \n",
       "6b384c0a008fcdc1 0                  6e13a8d5-0deb-4982-8f6a-601164f66a73   \n",
       "                 1                  36084d3b-9775-4e1b-87a6-c542507e0074   \n",
       "                 2                  a0149ea1-fb03-4f53-be3a-891702d05314   \n",
       "6fe4de290b8d7478 0                  def89f0c-d7d3-4c57-964a-4689cfda6ad3   \n",
       "                 1                  36084d3b-9775-4e1b-87a6-c542507e0074   \n",
       "                 2                  dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9   \n",
       "7d64a5556cec5907 0                  a23b9fae-a936-451e-b8d5-573ab30f08dd   \n",
       "                 1                  a3bb85cc-4987-415a-a1c6-e41f7c2a07d1   \n",
       "                 2                  207f6d67-3dbf-4aea-88dc-c9df94f43b43   \n",
       "c60037afa8f849c4 0                  dc8a928d-6cc7-4393-a0c5-3754e4cb8cb9   \n",
       "                 1                  207f6d67-3dbf-4aea-88dc-c9df94f43b43   \n",
       "                 2                  a23b9fae-a936-451e-b8d5-573ab30f08dd   \n",
       "d0a79204813cc2bb 0                  def89f0c-d7d3-4c57-964a-4689cfda6ad3   \n",
       "                 1                  a23b9fae-a936-451e-b8d5-573ab30f08dd   \n",
       "                 2                  207f6d67-3dbf-4aea-88dc-c9df94f43b43   \n",
       "f336ec694fdcbba5 0                  6e13a8d5-0deb-4982-8f6a-601164f66a73   \n",
       "                 1                  825177e1-6c77-4de7-bb02-c332e8dbaf54   \n",
       "                 2                  0511cc03-6907-47b7-a5b6-2e59816aa6b9   \n",
       "\n",
       "                                    document_score      label  score  \\\n",
       "context.span_id  document_position                                     \n",
       "0224631cead9f25d 0                        0.025658   relevant    1.0   \n",
       "                 1                        0.007090   relevant    1.0   \n",
       "                 2                        0.004895   relevant    1.0   \n",
       "1a81efb791e850df 0                        0.966074       None    NaN   \n",
       "                 1                        0.001705       None    NaN   \n",
       "                 2                        0.001625       None    NaN   \n",
       "1ae2af8f3837139e 0                        0.978433       None    NaN   \n",
       "                 1                        0.949401       None    NaN   \n",
       "                 2                        0.112510       None    NaN   \n",
       "1e9fbe7fd91c299d 0                        0.996719       None    NaN   \n",
       "                 1                        0.016507       None    NaN   \n",
       "                 2                        0.000792       None    NaN   \n",
       "4abdb3f2ef462568 0                        0.977319       None    NaN   \n",
       "                 1                        0.006412       None    NaN   \n",
       "                 2                        0.001090       None    NaN   \n",
       "5115f7ecca3a55ea 0                        0.988813       None    NaN   \n",
       "                 1                        0.050221       None    NaN   \n",
       "                 2                        0.007747       None    NaN   \n",
       "5551e37d1a9fdeef 0                        0.992121       None    NaN   \n",
       "                 1                        0.005779       None    NaN   \n",
       "                 2                        0.004970       None    NaN   \n",
       "6b384c0a008fcdc1 0                        0.995588   relevant    1.0   \n",
       "                 1                        0.018183  unrelated    0.0   \n",
       "                 2                        0.000928       None    NaN   \n",
       "6fe4de290b8d7478 0                        0.023789   relevant    1.0   \n",
       "                 1                        0.023743   relevant    1.0   \n",
       "                 2                        0.010969   relevant    1.0   \n",
       "7d64a5556cec5907 0                        0.957924       None    NaN   \n",
       "                 1                        0.398520       None    NaN   \n",
       "                 2                        0.006537       None    NaN   \n",
       "c60037afa8f849c4 0                        0.843321       None    NaN   \n",
       "                 1                        0.112517       None    NaN   \n",
       "                 2                        0.037068       None    NaN   \n",
       "d0a79204813cc2bb 0                        0.649439       None    NaN   \n",
       "                 1                        0.034376       None    NaN   \n",
       "                 2                        0.027596       None    NaN   \n",
       "f336ec694fdcbba5 0                        0.092397       None    NaN   \n",
       "                 1                        0.065734       None    NaN   \n",
       "                 2                        0.038693       None    NaN   \n",
       "\n",
       "                                                                          explanation  \n",
       "context.span_id  document_position                                                     \n",
       "0224631cead9f25d 0                  EXPLANATION: The question asks for information...  \n",
       "                 1                  EXPLANATION: The question asks for information...  \n",
       "                 2                  EXPLANATION: The question asks for information...  \n",
       "1a81efb791e850df 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "1ae2af8f3837139e 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "1e9fbe7fd91c299d 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "4abdb3f2ef462568 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "5115f7ecca3a55ea 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "5551e37d1a9fdeef 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "6b384c0a008fcdc1 0                  EXPLANATION: The question asks about a specifi...  \n",
       "                 1                  EXPLANATION: The question asks about a specifi...  \n",
       "                 2                                                               None  \n",
       "6fe4de290b8d7478 0                  EXPLANATION: The question asks for information...  \n",
       "                 1                  EXPLANATION: The question asks for information...  \n",
       "                 2                  EXPLANATION: The question asks for information...  \n",
       "7d64a5556cec5907 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "c60037afa8f849c4 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "d0a79204813cc2bb 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  \n",
       "f336ec694fdcbba5 0                                                               None  \n",
       "                 1                                                               None  \n",
       "                 2                                                               None  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f841a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "def _compute_ndcg(df: pd.DataFrame, k: int):\n",
    "    \"\"\"Compute NDCG@k in the presence of missing values\"\"\"\n",
    "    n = max(2, len(df))\n",
    "    eval_scores = np.zeros(n)\n",
    "    doc_scores = np.zeros(n)\n",
    "    eval_scores[: len(df)] = df.score\n",
    "    doc_scores[: len(df)] = df.document_score\n",
    "    try:\n",
    "        return ndcg_score([eval_scores], [doc_scores], k=k)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "ndcg_at_2 = pd.DataFrame(\n",
    "    {\"score\": final_df.groupby(\"context.span_id\").apply(_compute_ndcg, k=2)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56f60504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0224631cead9f25d</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1a81efb791e850df</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ae2af8f3837139e</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1e9fbe7fd91c299d</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4abdb3f2ef462568</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115f7ecca3a55ea</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551e37d1a9fdeef</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6b384c0a008fcdc1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6fe4de290b8d7478</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7d64a5556cec5907</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c60037afa8f849c4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d0a79204813cc2bb</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f336ec694fdcbba5</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score\n",
       "context.span_id        \n",
       "0224631cead9f25d    1.0\n",
       "1a81efb791e850df    NaN\n",
       "1ae2af8f3837139e    NaN\n",
       "1e9fbe7fd91c299d    NaN\n",
       "4abdb3f2ef462568    NaN\n",
       "5115f7ecca3a55ea    NaN\n",
       "5551e37d1a9fdeef    NaN\n",
       "6b384c0a008fcdc1    NaN\n",
       "6fe4de290b8d7478    1.0\n",
       "7d64a5556cec5907    NaN\n",
       "c60037afa8f849c4    NaN\n",
       "d0a79204813cc2bb    NaN\n",
       "f336ec694fdcbba5    NaN"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38831399",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_2 = pd.DataFrame(\n",
    "    {\n",
    "        \"score\": final_df.groupby(\"context.span_id\").apply(\n",
    "            lambda x: x.score[:2].sum(skipna=False) / 2\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8339330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0224631cead9f25d</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1a81efb791e850df</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ae2af8f3837139e</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1e9fbe7fd91c299d</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4abdb3f2ef462568</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115f7ecca3a55ea</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5551e37d1a9fdeef</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6b384c0a008fcdc1</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6fe4de290b8d7478</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7d64a5556cec5907</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c60037afa8f849c4</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d0a79204813cc2bb</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f336ec694fdcbba5</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score\n",
       "context.span_id        \n",
       "0224631cead9f25d    1.0\n",
       "1a81efb791e850df    NaN\n",
       "1ae2af8f3837139e    NaN\n",
       "1e9fbe7fd91c299d    NaN\n",
       "4abdb3f2ef462568    NaN\n",
       "5115f7ecca3a55ea    NaN\n",
       "5551e37d1a9fdeef    NaN\n",
       "6b384c0a008fcdc1    0.5\n",
       "6fe4de290b8d7478    1.0\n",
       "7d64a5556cec5907    NaN\n",
       "c60037afa8f849c4    NaN\n",
       "d0a79204813cc2bb    NaN\n",
       "f336ec694fdcbba5    NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_at_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59a0a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit = pd.DataFrame(\n",
    "    {\n",
    "        \"hit\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n",
    "            lambda x: x.score[:2].sum(skipna=False) > 0\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb5a69eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(dataframe=ndcg_at_2, eval_name=\"ndcg@2\"),\n",
    "    SpanEvaluations(dataframe=precision_at_2, eval_name=\"precision@2\"),\n",
    "    DocumentEvaluations(dataframe=final_df, eval_name=\"relevance\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiteLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
